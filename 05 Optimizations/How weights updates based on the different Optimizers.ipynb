{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD\n",
    "    theta0 = theta0 - lr * dtheta0\n",
    "    theta1 = theta1 - lr * dtheta1\n",
    "\n",
    "# Momentum\n",
    "    '''\n",
    "    below the code 'vw' and 'vb' are velocity variables to store oue momentum for every parameters.\n",
    "    The update of the velocity is given the old velocity value and new gradient Descent step.\n",
    "    also we decay our past velocity so that we only consider the most recent velocities with gamma =.9\n",
    "    '''    \n",
    "    vw = gamma * vw + lr * dtheta0\n",
    "    vb = gamma * vb + lr * dtheta1\n",
    "\n",
    "    theta0 = theta0 - vw\n",
    "    theta1 = theta1 - vb\n",
    "\n",
    "# NAG\n",
    "    '''\n",
    "    Nesterov Momentum add one little different bit to the momentum calculation.\n",
    "    Instead of calculating gradient of the current position, it calculates the gradient at the approximated new position.\n",
    "    It uses the next approximated position’s gradient with the hope that it will give us better \n",
    "    information when we’re taking the next step\n",
    "    \n",
    "    Here we are computing the gradient using model head approximated next state of our model parameters \\\n",
    "    that we calculated by adding the momentum to the current parameters.\n",
    "    \n",
    "    We know we’ll be using γV(t−1) for modifying the weights so, θ−γV(t−1)[theta0 = theta0 - vw] approximately \n",
    "    tells us the future locationNow, we’ll calculate the cost based on this future parameter rather than the current one.\n",
    "    \n",
    "    '''\n",
    "    vw = gamma * vw\n",
    "    vb = gamma * vb \n",
    "\n",
    "    theta0 = theta0 - vw\n",
    "    theta1 = theta1 - vb\n",
    "\n",
    "    vw = vw + eta * dtheta0\n",
    "    vb = vb + eta * dtheta1\n",
    "\n",
    "    theta0 = theta0 - vw\n",
    "    theta1 = theta1 - vb\n",
    "\n",
    "# AdaGrad\n",
    "    \n",
    "        '''\n",
    "        The problem with learning rate in Gradient Descent is that it’s constant and affecting all of our parameters\n",
    "        \n",
    "         What happen if we know that we should slow down or speed up?\n",
    "         \n",
    "         What happen if we know that we should accelerate more in this direction and decelerate in that direction?\n",
    "         \n",
    "         What we do is to accumulate the sum of squared of all of our parameters’ gradient, and use that to \n",
    "         normalize the learning rate alpha,\n",
    "         \n",
    "         so that now our alpha could be smaller or larger depending on how the past gradients behaved.\n",
    "         \n",
    "         parameters that updated a lot will be slowed down while parameters that received little updates will be have \n",
    "         bigger learning rate to accelerate the learning process.\n",
    "        \n",
    "        '''\n",
    "\n",
    "    vw = vw + dtheta0**2\n",
    "    vb = vb + dtheta1**2\n",
    "\n",
    "    theta0 = theta0 - (lr / np.sqrt(vw) + eps ) * dtheta0\n",
    "    theta1 = theta1 - (lr / np.sqrt(vb) + eps ) * dtheta1\n",
    "\n",
    "# RMSProp :RMSprop deals with the above issue by using a moving average of squared gradients to normalize the gradient.\n",
    "\n",
    "    '''\n",
    "    If you notice, at the gradient accumulation part in Adagrad cache[k] += grad[k]**2.\n",
    "    \n",
    "    it’s monotonically increasing (hint: sum and squared).this could be problematic as the learning rate will be monotonically \n",
    "    decreasing to the point that the learning stops altogether because of the very tiny learning rate.\n",
    "    \n",
    "    To combat that problem, RMSprop decay the past accumulated gradient, so only a portion of past gradients are considered. \n",
    "    Now, instead of considering all of the past gradients, RMSprop behaves like moving average.\n",
    "    \n",
    "    Here, we’re take gamma portion of past accumulated sum of squared gradient, \n",
    "    and take 1 - gamma portion of the current squared gradient. \n",
    "    \n",
    "    By doing this, the accumulated gradient won’t be aggresively monotonically increasing,\n",
    "    '''\n",
    "\n",
    "\n",
    "    vw = beta * vw + (1 - beta) * dtheta0**2\n",
    "    vb = beta * vb + (1 - beta) * dtheta1**2\n",
    "\n",
    "    theta0 = theta0 - (eta / np.sqrt(vw) + eps) * dtheta0\n",
    "    theta1 = theta1 - (eta / np.sqrt(vb) + eps) * dtheta1\n",
    "\n",
    "\n",
    "# Adam\n",
    "\n",
    "    '''\n",
    "    Adam tries to combine the best of both world of momentum and adaptive learning rate.\n",
    "    \n",
    "    we still retain some RMSprop’s codes, namely when we calculate R.\n",
    "    We also add some codes that are similar to how we compute momentum in the form of M\n",
    "    Then, for the parameters update, it’s the combination of momentum method and adaptive learning rate method:\n",
    "    add the momentum, and normalize the learning rate using the moving average squared gradient.\n",
    "    \n",
    "    Adam also has a bias correction mechanism, it’s calculated in m_k_hat and r_k_hat. \n",
    "    It’s useful to make the convergence faster, at several first iterations.\n",
    "    The reason is that we initialized M and R with zero, hence it will be biased toward zero in several first iterations.\n",
    "    \n",
    "    The solution is to correct the bias and get the unbiased estimate of M and R\n",
    "    \n",
    "    Adam also keeps an exponentially decaying average of past gradients M(t).\n",
    "    M(t) and V(t) are values of the first moment which is the Mean and the second \n",
    "    moment which is the uncentered variance of the gradients respectively.\n",
    "    \n",
    "    '''\n",
    "    mw = beta1 * mw + (1-beta1) * dtheta0\n",
    "    mb = beta1 * mb + (1-beta1) * dtheta1\n",
    "\n",
    "    vw = beta2 * vw + (1-beta2) * dtheta0**2\n",
    "    vb = beta2 * vb + (1-beta2) * dtheta1**2\n",
    "\n",
    "    mw_c = mw / (1 - np.power(beta1, num_updates))\n",
    "    mb_c = mb / (1 - np.power(beta1, num_updates))\n",
    "\n",
    "    vw_c = vw / (1 - np.power(beta2, num_updates))\n",
    "    vb_c = vb / (1 - np.power(beta2, num_updates))\n",
    "\n",
    "    theta0 = theta0 - (eta / np.sqrt(vw_c) + eps) * mw_c\n",
    "    theta1 = theta1 - (eta / np.sqrt(vb_c) + eps) * mb_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 01 \n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates\n",
    "    param := param - learning_rate * gradient\n",
    "\n",
    "#02\n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates with momentum\n",
    "    velocity := momentum * velocity - learning_rate * gradient\n",
    "    param := param + velocity\n",
    "    \n",
    "#03 \n",
    "\n",
    "Stochastic Gradient Descent (SGD) updates with Nesterov momentum\n",
    "    velocity := momentum * velocity - learning_rate * gradient\n",
    "    param := param + momentum * elocity - learning_rate * gradient\n",
    "    \n",
    "#04 \n",
    "\n",
    "Adagrad updates:\n",
    "    Scale learning rates by dividing with the square root of accumulated squared gradients.\n",
    "\n",
    "#05\n",
    "\n",
    "RMSProp:\n",
    "    Scale learning rates by dividing with the moving average of the root mean squared (RMS) gradients\n",
    "\n",
    "#06\n",
    "Adadelta updates:\n",
    "    Scale learning rates by the ratio of accumulated gradients to accumulated updates.\n",
    "   \n",
    "#07\n",
    "Adamax:\n",
    "    This is a variant of of the Adam algorithm based on the infinity norm.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
