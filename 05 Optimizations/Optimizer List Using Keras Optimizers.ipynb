{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers Supported in Keras\n",
    "    - SGD \n",
    "        - Frequent updates of model parameters hence, converges in less time\n",
    "        \n",
    "    - Momonetum \n",
    "        - Acceruate SGD and reduces the oscillations and high variance of the parameters.\n",
    "        \n",
    "    - AdaGrad \n",
    "        - Implement learning rate changes for each training parameter.\n",
    "        \n",
    "    - Adadelta and RMSProp \n",
    "        - It is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it.\n",
    "        \n",
    "    - Adam : \n",
    "        - combination of Momentum + RMSProp\n",
    "        \n",
    "    - AdamMax\n",
    "        - This is a variant of of the Adam algorithm based on the infinity norm.\n",
    "        \n",
    "    - Nadam : Nesterov-accelerated Adaptive Moment Estimation\n",
    "        - Nadam is an extension of the Adam version of gradient descent that incorporates Nesterov momentum.\n",
    "        \n",
    "    - Ftrl\n",
    "        - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporatent Parameters to remember while working with Keras Optimizers\n",
    "\n",
    "'''\n",
    "Learning rate: \n",
    "    - step size we take to reach the minimum of the objective function.\n",
    "    - How large learning rates result in unstable training and tiny rates result in a failure to train\n",
    "\n",
    "momentum :\n",
    "    - Accelerate SGD to converge the optimization process faster.\n",
    "    - Momentum was invented for reducing high variance in SGD (weights updated for every values in the dataset)\n",
    "    - Best value 0.5 to 0.9\n",
    "    \n",
    "rho : \n",
    "    - Exponentially weighted average over the square of the gradients.\n",
    "    - also called as Gradient moving average decay factor\n",
    "    - rho should be between 0 and 1. \n",
    "    - rho close to 1 will decay the moving average slowly and a value close to 0 will decay the moving average fast\n",
    "    \n",
    "nesterov : Whether to apply Nesterov momentum. Defaults to False\n",
    "\n",
    "epsilon : Is a very small number to prevent any division by zero in the implementation\n",
    "\n",
    "decay :\n",
    "    - With decay value, the learning rate is calculated each update.\n",
    "    - lrate = initial_lrate * (1 / (1 + decay * iteration))\n",
    "    - Decay is the decay rate which is greater than zero and iteration is the current update number.\n",
    "    - 'decay' decays the learning rate over time, so we can move even closer to the local minimum in the end of training.\n",
    "\n",
    "centered : \n",
    "\n",
    "beta_1 : Exponential decay rate for the first moment estimates.\n",
    "\n",
    "beta_2 : Exponential decay rate for the weighted infinity norm estimates.\n",
    "\n",
    "amsgrad :\n",
    "\n",
    "learning_rate_power :\n",
    "\n",
    "initial_accumulator_value : \n",
    "    - In Adagrad equation denominator is  accumulation of the squared gradients.\n",
    "    - Here we can set the Initial value for the same\n",
    "    - It must be non negative value\n",
    "\n",
    "clipnorm : \n",
    "    - Used for Gradient Clippng \n",
    "    - The gradients can be rescaled to have a vector norm (magnitude or length) of 1.0\n",
    "\n",
    "clipvalue :\n",
    "    - Used for Gradient Clippng \n",
    "    - ex : clipvalue:5 =  Clip the gradient to the range [-5 to 5]\n",
    "\n",
    "l1_regularization_strength :\n",
    "\n",
    "l2_regularization_strength :\n",
    "\n",
    "l2_shrinkage_regularization_strength :\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "List of parameters for Optimizations in Keras\n",
    "\n",
    "    - SGD\n",
    "        - Learning rate\n",
    "        - momentum\n",
    "        - nesterov = True/False\n",
    "        - name\n",
    "        - decay\n",
    "        - clipnorm :True/False\n",
    "        - clipvalue\n",
    "    \n",
    "    - Adgrad\n",
    "        - learning_rate\n",
    "        - initial_accumulator_value\n",
    "        - epsilon\n",
    "        - name\n",
    "        - decay\n",
    "        - clipnorm\n",
    "        - clipvalue\n",
    "    \n",
    "    - Adadelta\n",
    "        - learning_rate\n",
    "        - rho\n",
    "        - epsilon\n",
    "        - name\n",
    "        - decay\n",
    "        - clipnorm = rue/False\n",
    "        - clipvalue\n",
    "    \n",
    "    - RMSProp\n",
    "        - learning_rate\n",
    "        - rho\n",
    "        - momentum\n",
    "        - epsilon\n",
    "        - centered = True/False\n",
    "        - name\n",
    "        - clipnorm = rue/False\n",
    "        - clipvalue\n",
    "    \n",
    "    - Adam\n",
    "        - learning_rate\n",
    "        - beta_1\n",
    "        - beta_2\n",
    "        - epsilon\n",
    "        - amsgrad\n",
    "        - name\n",
    "        - decay\n",
    "        - clipnorm = True/False\n",
    "        - clipvalue\n",
    "\n",
    "    - AdaMax\n",
    "        - learning_rate\n",
    "        - beta_1\n",
    "        - beta_2\n",
    "        - epsilon\n",
    "        - name\n",
    "        - decay\n",
    "        - clipnorm = True/False\n",
    "        - clipvalue\n",
    "\n",
    "    - Nadam\n",
    "        - learning_rate\n",
    "        - beta_1\n",
    "        - beta_2\n",
    "        - epsilon\n",
    "        - name\n",
    "        - decay\n",
    "        - clipnorm = True/False\n",
    "        - clipvalue\n",
    "    \n",
    "    - Ftrl\n",
    "        - learning_rate\n",
    "        - learning_rate_power\n",
    "        - initial_accumulator_value\n",
    "        - l1_regularization_strength\n",
    "        - l2_regularization_strength\n",
    "        - name\n",
    "        - l2_shrinkage_regularization_strength\n",
    "        - decay\n",
    "        - clipnorm = True/False\n",
    "        - clipvalue\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers implementation in Keras\n",
    "\n",
    "'''\n",
    "#01\n",
    "\n",
    "Stochastic Gradient Descent(SGD) :\n",
    "\n",
    "# Create Optimixer Instance\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
    "\n",
    "\n",
    "- Compute:\n",
    "    - theta(t+1) = theta(t) - learning_rate * gradient\n",
    "    \n",
    "- Compute : (nesterov=True)\n",
    "    - v(t+1) = momentum * v(t) - learning_rate * gradient\n",
    "    - theta(t+1) = theta(t) + v(t+1)\n",
    "    \n",
    "- Constant learning rate is the default learning rate schedule in SGD optimizer\n",
    "- Momentum and decay rate are both set to zero by default\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weight values after last iteration\n",
      "m=37.35943603515625 and b =-5.0799946784973145\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    " Y = mx + b\n",
    " \n",
    " weights = [w,b]\n",
    " \n",
    " cost = sum(abs(Y - (m*x + b))\n",
    " \n",
    "'''\n",
    "X = [30, 35, 37, 59, 70, 76, 88, 100 ]\n",
    "Y = [1100, 1423, 1377, 1800, 2304, 2588, 3495, 4839]\n",
    "\n",
    "m = tf.Variable(1.0) \n",
    "b = tf.Variable(1.0) \n",
    "\n",
    "iteration = []\n",
    "loss = []\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    iteration.append(epoch)\n",
    "    \n",
    "    opt.minimize(lambda : sum(abs(Y - (m* X+ b))),var_list=[m,b])\n",
    "    \n",
    "    loss.append(sum(abs(Y - (m* X+ b))).numpy())\n",
    "    \n",
    "print(\"Final weight values after last iteration\")\n",
    "print(\"m={0} and b ={1}\".format(m.numpy(),b.numpy()))\n",
    "\n",
    "plt.plot(iteration,loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#02\n",
    "\n",
    "Adagrad:\n",
    "\n",
    "opt = tf.keras.optimizers.Adagrad(learning_rate=0.001,initial_accumulator_value=0.1,epsilon=1e-07,name=\"Adagrad\") \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#03\n",
    "\n",
    "RMSProp\n",
    "\n",
    "# Create Optimizer Insatnce\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001,rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,name='RMSprop')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#04\n",
    "\n",
    "Adadelta : Adaptive delta\n",
    "\n",
    "# Create Optimizer Instance\n",
    "opt = tf.keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta') \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#05\n",
    "\n",
    "Adam Optimizer : Adaptive Moment Estimation\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#06\n",
    "\n",
    "AdaMax Optimizer :\n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,name='Adamax')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#07\n",
    "\n",
    "NAdam Optimizer  : Nesterov and Adam optimizer\n",
    "\n",
    "opt = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,name='Nadam')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#08\n",
    "\n",
    "Ftrl Optimizer :\n",
    "\n",
    "opt =  tf.keras.optimizers.Ftrl(learning_rate=0.001,learning_rate_power=-0.5,initial_accumulator_value=0.1,\n",
    "                                l1_regularization_strength=0.0,l2_regularization_strength=0.0,name=\"Ftrl\",\n",
    "                                l2_shrinkage_regularization_strength=0.0,beta=0.0)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
