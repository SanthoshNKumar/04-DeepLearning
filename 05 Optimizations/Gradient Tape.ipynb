{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Tape :  Automatic differentiation\n",
    "\n",
    "# Note that only tensors with real or complex dtypes are differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1:\n",
    "\n",
    "# function : x**2 \n",
    "# y = x**2   # xSquare # derive if function X**2 is 2x if x = 3 the derivative output is 6\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "    \n",
    "dy_dx = tape.gradient(y, x)  # First order derivative\n",
    "\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.0\n"
     ]
    }
   ],
   "source": [
    "# Automatically Watching Variables\n",
    "\n",
    "# If x were a trainable variable instead of a constant, there would be no need to tell the tape to watch it\n",
    "\n",
    "x = tf.Variable(6.0, trainable=True)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**3\n",
    "\n",
    "print(tape.gradient(y, x).numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d95bcc576523>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "# If we were to re-run this replacing the first line with: x = tf.constant(3.0)\n",
    "\n",
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**3\n",
    "\n",
    "print(tape.gradient(y, x).numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch_accessed_variables=False \n",
    "#    - If we don’t want GradientTape to watch all trainable variables automatically\n",
    "# Disabling watch_accessed_variables gives us fine control over what variables we want to watch\n",
    "\n",
    "x = tf.Variable(3.0, trainable=True)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "    y = x**3\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher-Order Derivatives\n",
    "\n",
    "x = tf.Variable(2.0) # first oder output will be 6\n",
    "\n",
    "with tf.GradientTape() as tape1:\n",
    "    tape1.watch(x)\n",
    "    with tf.GradientTape() as tape2:\n",
    "        tape2.watch(x)\n",
    "        y = 3*x**2\n",
    "    dy_dx = tape2.gradient(y,x)     # tape1 :First order derivative\n",
    "d2y_d2x = tape1.gradient(dy_dx, x)   # tape : Second Order derivative\n",
    "\n",
    "print(dy_dx)\n",
    "\n",
    "print(d2y_d2x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "\n",
    "x = tf.Variable([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    \n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "    \n",
    "[dl_dw, dl_db]= tape.gradient(loss,[w,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# the GradientTape will not releases all the information stored inside of it for computational purposes.\n",
    "# Default : persistent=False\n",
    "\n",
    "# If we want to see all the varibales information,we can set persistent=True\n",
    "\n",
    "a = tf.Variable(6.0, trainable=True)\n",
    "b = tf.Variable(2.0, trainable=True)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y1 = a ** 2\n",
    "    y2 = b ** 3\n",
    "                                                                                                                                                                                                                                                                                                                                                \n",
    "print(tape.gradient(y1, a).numpy())\n",
    "print(tape.gradient(y2, b).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0\n"
     ]
    }
   ],
   "source": [
    "# stop_recording\n",
    "# tape.stop_recording() temporarily pauses the tapes recording, leading to greater computation speed.\n",
    "\n",
    "x = tf.Variable(3.0, trainable=True)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**3\n",
    "    with tape.stop_recording():\n",
    "        print(tape.gradient(y, x).numpy()) # -> 27.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-9b71a10b506b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "# By default, GradientTape doesn’t track constants, so we must instruct it to with: tape.watch(variable)\n",
    "# Below the code throwing error becuse tape.watch(x) commented\n",
    "\n",
    "x = tf.constant(5.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    #tape.watch(x)\n",
    "    y = x**3\n",
    "    \n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Getting a gradient of None\n",
    "\n",
    "# When a target is not connected to a source you will get a gradient of None.\n",
    "\n",
    "x = tf.Variable(2.)\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = y*y\n",
    "\n",
    "print(tape.gradient(z,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Zeros instead of None\n",
    "\n",
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Other Methods\n",
    "\n",
    ".jacobian: “Computes the jacobian using operations recorded in context of this tape.”\n",
    "   \n",
    ".batch_jacobian: “Computes and stacks per-example jacobians.”\n",
    "\n",
    ".reset: “Clears all information stored in this tape.”\n",
    "\n",
    ".watched_variables: “Returns variables watched by this tape in order of\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=220, shape=(), dtype=float32, numpy=12.0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GradientTapes can be nested to compute higher-order derivatives. \n",
    "\n",
    "x = tf.Variable(2.0) # first oder output will be 6\n",
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    y = 3*x**2\n",
    "    \n",
    "tape.gradient(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#  Replaced a variable with a tensor\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "for epoch in range(2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = x+1\n",
    "        \n",
    "    print(type(x).__name__, \":\", tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
    "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
    "\n",
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "\n",
    "\n",
    "x = tf.Variable([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    \n",
    "    y = x @ w + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "    \n",
    "[dl_dw, dl_db]= tape.gradient(loss,[w,b])\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': <tf.Tensor: id=315, shape=(3, 2), dtype=float32, numpy=\n",
       " array([[-1.47831  , -2.9592826],\n",
       "        [-2.95662  , -5.9185653],\n",
       "        [-4.43493  , -8.877848 ]], dtype=float32)>,\n",
       " 'b': <tf.Tensor: id=313, shape=(2,), dtype=float32, numpy=array([-1.47831  , -2.9592826], dtype=float32)>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense/kernel:0, shape: (3, 2)\n",
      "dense/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "# Gradients with respect to a model\n",
    "\n",
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    \n",
    "    # Forward pass\n",
    "    y = layer(x)\n",
    "    \n",
    "    loss = tf.reduce_mean(y**2)\n",
    "    \n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables) \n",
    "\n",
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Example :\n",
    "\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = (x0**2) + (x1**2) + (x2**2)\n",
    "    \n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-10.0, 10.0, 200+1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = tf.nn.sigmoid(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=431, shape=(201,), dtype=float32, numpy=\n",
       "array([4.54166766e-05, 5.02143921e-05, 5.54590479e-05, 6.13294178e-05,\n",
       "       6.77360949e-05, 7.47982340e-05, 8.26946052e-05, 9.14251650e-05,\n",
       "       1.01049460e-04, 1.11627036e-04, 1.23396196e-04, 1.36356830e-04,\n",
       "       1.50747219e-04, 1.66567232e-04, 1.84025266e-04, 2.03389267e-04,\n",
       "       2.24718591e-04, 2.48400233e-04, 2.74523190e-04, 3.03384935e-04,\n",
       "       3.35312623e-04, 3.70424765e-04, 4.09375789e-04, 4.52343491e-04,\n",
       "       4.99922200e-04, 5.52467944e-04, 6.10514835e-04, 6.74626499e-04,\n",
       "       7.45455211e-04, 8.23771697e-04, 9.10197268e-04, 1.00573874e-03,\n",
       "       1.11131265e-03, 1.22783449e-03, 1.35663373e-03, 1.49891921e-03,\n",
       "       1.65598630e-03, 1.82954292e-03, 2.02120445e-03, 2.23287800e-03,\n",
       "       2.46652518e-03, 2.72454484e-03, 3.00929835e-03, 3.32372868e-03,\n",
       "       3.67067871e-03, 4.05353727e-03, 4.47600102e-03, 4.94215591e-03,\n",
       "       5.45618078e-03, 6.02310384e-03, 6.64806180e-03, 7.33687775e-03,\n",
       "       8.09598900e-03, 8.93205591e-03, 9.85276140e-03, 1.08662117e-02,\n",
       "       1.19813727e-02, 1.32077243e-02, 1.45557523e-02, 1.60367247e-02,\n",
       "       1.76627338e-02, 1.94466617e-02, 2.14024633e-02, 2.35448945e-02,\n",
       "       2.58895922e-02, 2.84529887e-02, 3.12524401e-02, 3.43058780e-02,\n",
       "       3.76318097e-02, 4.12490070e-02, 4.51766551e-02, 4.94335108e-02,\n",
       "       5.40381111e-02, 5.90077043e-02, 6.43582344e-02, 7.01037124e-02,\n",
       "       7.62549564e-02, 8.28195587e-02, 8.98003504e-02, 9.71946865e-02,\n",
       "       1.04993574e-01, 1.13180287e-01, 1.21729314e-01, 1.30605757e-01,\n",
       "       1.39763847e-01, 1.49146467e-01, 1.58684954e-01, 1.68298364e-01,\n",
       "       1.77894473e-01, 1.87369928e-01, 1.96611941e-01, 2.05500335e-01,\n",
       "       2.13909701e-01, 2.21712887e-01, 2.28784278e-01, 2.35003725e-01,\n",
       "       2.40260750e-01, 2.44458303e-01, 2.47516572e-01, 2.49376044e-01,\n",
       "       2.50000000e-01, 2.49376029e-01, 2.47516572e-01, 2.44458303e-01,\n",
       "       2.40260720e-01, 2.35003725e-01, 2.28784233e-01, 2.21712887e-01,\n",
       "       2.13909701e-01, 2.05500260e-01, 1.96611926e-01, 1.87369838e-01,\n",
       "       1.77894443e-01, 1.68298364e-01, 1.58684835e-01, 1.49146467e-01,\n",
       "       1.39763758e-01, 1.30605787e-01, 1.21729344e-01, 1.13180175e-01,\n",
       "       1.04993574e-01, 9.71946642e-02, 8.98003504e-02, 8.28195363e-02,\n",
       "       7.62549862e-02, 7.01037124e-02, 6.43582866e-02, 5.90077341e-02,\n",
       "       5.40381111e-02, 4.94334847e-02, 4.51766551e-02, 4.12489809e-02,\n",
       "       3.76317799e-02, 3.43058519e-02, 3.12523581e-02, 2.84529887e-02,\n",
       "       2.58896220e-02, 2.35448945e-02, 2.14025490e-02, 1.94466617e-02,\n",
       "       1.76627338e-02, 1.60366967e-02, 1.45557523e-02, 1.32077541e-02,\n",
       "       1.19813140e-02, 1.08662117e-02, 9.85279121e-03, 8.93205591e-03,\n",
       "       8.09593033e-03, 7.33687775e-03, 6.64809160e-03, 6.02304516e-03,\n",
       "       5.45615098e-03, 4.94218525e-03, 4.47600102e-03, 4.05353727e-03,\n",
       "       3.67070828e-03, 3.32375849e-03, 3.00938706e-03, 2.72445590e-03,\n",
       "       2.46652518e-03, 2.23290781e-03, 2.02120445e-03, 1.82954292e-03,\n",
       "       1.65598630e-03, 1.49894902e-03, 1.35666353e-03, 1.22783449e-03,\n",
       "       1.11134246e-03, 1.00570894e-03, 9.10167466e-04, 8.23771697e-04,\n",
       "       7.45455211e-04, 6.74566953e-04, 6.10514835e-04, 5.52467944e-04,\n",
       "       4.99952002e-04, 4.52432781e-04, 4.09435335e-04, 3.70424765e-04,\n",
       "       3.35342396e-04, 3.03414738e-04, 2.74463615e-04, 2.48370430e-04,\n",
       "       2.24718591e-04, 2.03389267e-04, 1.84025266e-04, 1.66567232e-04,\n",
       "       1.50717431e-04, 1.36356830e-04, 1.23366393e-04, 1.11627036e-04,\n",
       "       1.01019665e-04, 9.14251650e-05, 8.27244003e-05, 7.47982340e-05,\n",
       "       6.77062926e-05, 6.12102231e-05, 5.55484439e-05, 5.01845934e-05,\n",
       "       4.54166766e-05], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x, y, label='y')\n",
    "plt.plot(x, dy_dx, label='dy/dx')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[2448.9119, 2414.0479, 2379.4338, 2345.07, 231...</td>\n",
       "      <td>3.535200e-01</td>\n",
       "      <td>5.135185e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[1520.0896, 171.03984, 4243.841, 13738.493, 28...</td>\n",
       "      <td>5.206655e+01</td>\n",
       "      <td>1.101167e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[7688.501, 154954.62, 489442.2, 1011151.1, 172...</td>\n",
       "      <td>-3.059587e+02</td>\n",
       "      <td>-3.768410e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[71910.3, 5829986.0, 20801920.0, 44987708.0, 7...</td>\n",
       "      <td>2.146375e+03</td>\n",
       "      <td>3.181609e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[4630447.5, 283166080.0, 992453500.0, 21324928...</td>\n",
       "      <td>-1.467569e+04</td>\n",
       "      <td>-2.101847e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[209230980.0, 13261590000.0, 46592573000.0, 10...</td>\n",
       "      <td>1.006942e+05</td>\n",
       "      <td>1.451482e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[9896471000.0, 624168140000.0, 2192191900000.0...</td>\n",
       "      <td>-6.905621e+05</td>\n",
       "      <td>-9.943101e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[465155560000.0, 29357091000000.0, 10311212000...</td>\n",
       "      <td>4.736196e+06</td>\n",
       "      <td>6.820731e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[21882160000000.0, 1380907800000000.0, 4850188...</td>\n",
       "      <td>-3.248273e+07</td>\n",
       "      <td>-4.677787e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[1029273340000000.0, 6.4954745e+16, 2.2814198e...</td>\n",
       "      <td>2.227799e+08</td>\n",
       "      <td>3.208234e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Loss             a  \\\n",
       "0  [2448.9119, 2414.0479, 2379.4338, 2345.07, 231...  3.535200e-01   \n",
       "1  [1520.0896, 171.03984, 4243.841, 13738.493, 28...  5.206655e+01   \n",
       "2  [7688.501, 154954.62, 489442.2, 1011151.1, 172... -3.059587e+02   \n",
       "3  [71910.3, 5829986.0, 20801920.0, 44987708.0, 7...  2.146375e+03   \n",
       "4  [4630447.5, 283166080.0, 992453500.0, 21324928... -1.467569e+04   \n",
       "5  [209230980.0, 13261590000.0, 46592573000.0, 10...  1.006942e+05   \n",
       "6  [9896471000.0, 624168140000.0, 2192191900000.0... -6.905621e+05   \n",
       "7  [465155560000.0, 29357091000000.0, 10311212000...  4.736196e+06   \n",
       "8  [21882160000000.0, 1380907800000000.0, 4850188... -3.248273e+07   \n",
       "9  [1029273340000000.0, 6.4954745e+16, 2.2814198e...  2.227799e+08   \n",
       "\n",
       "              b  \n",
       "0  5.135185e-01  \n",
       "1  1.101167e+01  \n",
       "2 -3.768410e+01  \n",
       "3  3.181609e+02  \n",
       "4 -2.101847e+03  \n",
       "5  1.451482e+04  \n",
       "6 -9.943101e+04  \n",
       "7  6.820731e+05  \n",
       "8 -4.677787e+06  \n",
       "9  3.208234e+07  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Training data\n",
    "x_train = np.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_train = np.asarray([i*10+5 for i in x_train]) # y = 10x+5\n",
    "\n",
    "pd.DataFrame({\"X\":x_train,\"Y\":y_train})\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "loss = []\n",
    "a_a = []\n",
    "b_b = []\n",
    "\n",
    "# Trainable variables\n",
    "a = tf.Variable(random.random(), trainable=True)\n",
    "b = tf.Variable(random.random(), trainable=True)\n",
    "\n",
    "for steps in range(10): \n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        # Function : (a * x_data) + b\n",
    "        two_gate = tf.add(tf.multiply(a,x_train),b)\n",
    "        \n",
    "        # Define loss\n",
    "        curr_loss = tf.square(tf.subtract(two_gate,50))\n",
    "        \n",
    "        a_a.append(a.numpy())\n",
    "        b_b.append(b.numpy())\n",
    "        \n",
    "        grads = t.gradient(curr_loss, [a,b])\n",
    "        optimizer.apply_gradients(zip(grads,[a,b]))\n",
    "    \n",
    "        loss.append(curr_loss.numpy())\n",
    "        \n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(data = {'Loss': loss,'a':a_a,'b':b_b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X    Y\n",
       "0    0    2\n",
       "1    1   16\n",
       "2    2   42\n",
       "3    3   80\n",
       "4    4  130\n",
       "5    5  192\n",
       "6    6  266\n",
       "7    7  352\n",
       "8    8  450\n",
       "9    9  560\n",
       "10  10  682"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polynomial Regression\n",
    "\n",
    "# Training data\n",
    "x_train = np.asarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_train = np.asarray([6*i**2 + 8*i + 2 for i in x_train]) # y = 6x^2 + 8x + 2\n",
    "\n",
    "pd.DataFrame({'X':x_train,'Y':y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[2475.923, 2371.8242, 2206.8738, 1987.9684, 17...</td>\n",
       "      <td>3.333656e-01</td>\n",
       "      <td>7.239059e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[2475.923, 22720.342, 479937.56, 2484939.8, 78...</td>\n",
       "      <td>1.707756e+02</td>\n",
       "      <td>2.971572e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[2475.923, 9661927000.0, 138442440000.0, 67469...</td>\n",
       "      <td>-8.776905e+04</td>\n",
       "      <td>-1.047629e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[2475.923, 2539728500000000.0, 3.6413227e+16, ...</td>\n",
       "      <td>4.501550e+07</td>\n",
       "      <td>5.380274e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[2475.923, 6.680948e+20, 9.578762e+21, 4.66866...</td>\n",
       "      <td>-2.308804e+10</td>\n",
       "      <td>-2.759485e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>[2475.923, 1.7574711e+26, 2.5197618e+27, 1.228...</td>\n",
       "      <td>1.184165e+13</td>\n",
       "      <td>1.415315e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>[2475.923, 4.6231524e+31, 6.628412e+32, 3.2306...</td>\n",
       "      <td>-6.073475e+15</td>\n",
       "      <td>-7.259024e+14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>[2475.923, 1.2161533e+37, 1.7436514e+38, inf, ...</td>\n",
       "      <td>3.115030e+18</td>\n",
       "      <td>3.723087e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>[2475.923, inf, inf, inf, inf, inf, inf, inf, ...</td>\n",
       "      <td>-1.597671e+21</td>\n",
       "      <td>-1.909538e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>[2475.923, inf, inf, inf, inf, inf, inf, inf, ...</td>\n",
       "      <td>8.194308e+23</td>\n",
       "      <td>9.793846e+22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Loss             a  \\\n",
       "0  [2475.923, 2371.8242, 2206.8738, 1987.9684, 17...  3.333656e-01   \n",
       "1  [2475.923, 22720.342, 479937.56, 2484939.8, 78...  1.707756e+02   \n",
       "2  [2475.923, 9661927000.0, 138442440000.0, 67469... -8.776905e+04   \n",
       "3  [2475.923, 2539728500000000.0, 3.6413227e+16, ...  4.501550e+07   \n",
       "4  [2475.923, 6.680948e+20, 9.578762e+21, 4.66866... -2.308804e+10   \n",
       "5  [2475.923, 1.7574711e+26, 2.5197618e+27, 1.228...  1.184165e+13   \n",
       "6  [2475.923, 4.6231524e+31, 6.628412e+32, 3.2306... -6.073475e+15   \n",
       "7  [2475.923, 1.2161533e+37, 1.7436514e+38, inf, ...  3.115030e+18   \n",
       "8  [2475.923, inf, inf, inf, inf, inf, inf, inf, ... -1.597671e+21   \n",
       "9  [2475.923, inf, inf, inf, inf, inf, inf, inf, ...  8.194308e+23   \n",
       "\n",
       "              b  \n",
       "0  7.239059e-01  \n",
       "1  2.971572e+01  \n",
       "2 -1.047629e+04  \n",
       "3  5.380274e+06  \n",
       "4 -2.759485e+09  \n",
       "5  1.415315e+12  \n",
       "6 -7.259024e+14  \n",
       "7  3.723087e+17  \n",
       "8 -1.909538e+20  \n",
       "9  9.793846e+22  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(0.01)\n",
    "\n",
    "loss = []\n",
    "a_a = []\n",
    "b_b = []\n",
    "\n",
    "# Trainable variables\n",
    "a = tf.Variable(random.random(), trainable=True)\n",
    "b = tf.Variable(random.random(), trainable=True)\n",
    "c = tf.Variable(random.random(), trainable=True)\n",
    "\n",
    "for steps in range(10): \n",
    "    with tf.GradientTape() as t:\n",
    "        \n",
    "        # Make prediction\n",
    "        y_pred = a*x_train**2 + b*x_train + c\n",
    "        \n",
    "        # Calculate loss\n",
    "        curr_loss = tf.square(tf.subtract(y_pred,50))\n",
    "        \n",
    "    \n",
    "        #print(tf.math.reduce_mean(curr_loss).numpy())\n",
    "        \n",
    "        a_a.append(a.numpy())\n",
    "        b_b.append(b.numpy())\n",
    "        \n",
    "        grads = t.gradient(curr_loss, [a,b])\n",
    "        optimizer.apply_gradients(zip(grads,[a,b]))\n",
    "    \n",
    "        loss.append(curr_loss.numpy())\n",
    "        \n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(data = {'Loss': loss,'a':a_a,'b':b_b})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
