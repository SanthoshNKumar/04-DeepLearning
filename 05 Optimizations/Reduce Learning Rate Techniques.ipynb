{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reduce learning rate as the training progresses\n",
    "    - When training deep neural networks, it is often useful to reduce learning rate as the training progresses\n",
    "    \n",
    "    - It can be done using below the methods\n",
    "        - Learning rate schedules\n",
    "        - Adaptive learning rate methods\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Learning Rate Schedule :\n",
    "\n",
    "    - common learinng rate schedules\n",
    "            - Time-based decay\n",
    "            - Step decay \n",
    "            - Exponential decay\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Time Based Decay\n",
    "        - lr = lr0 * 1/(1 + decay * epoch)\n",
    "        - lr = leanring rate \n",
    "        - lr0 = previous or initial lrearning rate\n",
    "        - decay = hyperparameter\n",
    "        - epoch = Iteration vaue\n",
    "\n",
    "\n",
    "Note : When the decay argument is zero (the default), this has no effect on the learning rate.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Step Decay or Drop-Based Learning Rate Decay \n",
    "        - Step decay schedule drops the learning rate by a factor every few epochs. The mathematical form of step decay is\n",
    "        - lr = lr0 * DropRate^floor(Epoch / EpochDrop)\n",
    "            - DropRate is the amount that the learning rate is modified each time it is changed such as 0.5\n",
    "            - Epoch is the current epoch number and EpochDrop is how often to change the learning rate such as 10\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Exponential Decay  : Exponentially decreases the learning rate over time from starting point\n",
    "        - lr = lr0 * e^(−decay_t)\n",
    "            - lr0 = initiali learning rate\n",
    "            - decay = decay hyperparameter\n",
    "            - t = iteration number\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Polynominal decay\n",
    "        - decay = (1 - (epoch / float(self.maxEpochs))) ** power\n",
    "                - maxEpochs : The total number of epochs we’ll be training for\n",
    "                - power : The power/exponent of the polynomial\n",
    "\n",
    "        - lr = lr0 * decay\n",
    "                -  lr0 = initiali learning rate\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Adaptive Learning Rate Methods\n",
    "        - The challenge of using learning rate schedules is that their hyperparameters have to be defined in \n",
    "          advance and they depend heavily on the type of model and problem.\n",
    "        - Another problem is that the same learning rate is applied to all parameter updates.\n",
    "\n",
    "    - Optimizers Supported by Adaptive Learning Rate Methods\n",
    "        - Adagrad\n",
    "        - Adadelta\n",
    "        - RMSprop\n",
    "        - Adam\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note \n",
    " - Adaptive learning rate methods demonstrate better performance than learning rate schedules\n",
    " - We can also use LearningRateScheduler in Keras to create custom learning rate schedules\n",
    " \n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Simple Model\n",
    "\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# load dataset\n",
    "dataframe = read_csv(\"ionosphere.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=34, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant learning rate\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Based Learning Rate Decay\n",
    "\n",
    "# Compile model\n",
    "epochs = 50\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / epochs\n",
    "momentum = 0.8\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=epochs, batch_size=28, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-Based Learning Rate Decay\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    " \n",
    "# Compile model\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.0, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential decay\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    decay = 0.1\n",
    "    return initial_learning_rate * math.exp(-decay*epoch)\n",
    "\n",
    "# Fit the model to the training data\n",
    "history_exp_decay = model.fit( X_train, y_train, epochs=100,validation_split=0.2,\n",
    "                              batch_size=64, callbacks=[LearningRateScheduler(lr_exp_decay, verbose=1)],\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
