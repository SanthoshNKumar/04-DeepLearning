{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03 Getting the Senetnce embedding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "evzBd9rDX6Qu"
      },
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbgzgDd3YGnB",
        "outputId": "61e964f5-eaa4-4228-da79-9f111259b23b"
      },
      "source": [
        "# load the model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF8KRSU_YVHU",
        "outputId": "4084faed-fa54-49ba-9522-67590f96c174"
      },
      "source": [
        "sentence = \"I Love Mysore and i leave in Krishnarajanagar\"\n",
        "\n",
        "# step 1 : Tokenize the sentence\n",
        "\n",
        "tokens = [x for x in sentence.lower().split()]\n",
        "tokens"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'mysore', 'and', 'i', 'leave', 'in', 'krishnarajanagar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBMkqSEaaEVz",
        "outputId": "550ba067-30b9-40ad-8d48-399c019c1729"
      },
      "source": [
        "# step 2: Add [CLS] and [SEP] tokens:\n",
        "\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "print(\" Tokens are \\n {} \".format(tokens))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Tokens are \n",
            " ['[CLS]', 'i', 'love', 'mysore', 'and', 'i', 'leave', 'in', 'krishnarajanagar', '[SEP]'] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZJHuhvAcyN9",
        "outputId": "a76c326a-9f6e-4fc6-bbc9-db4be5287f8c"
      },
      "source": [
        "# Step 3: Padding the input:\n",
        "\n",
        "T=15\n",
        "padded_tokens=tokens +['[PAD]' for _ in range(T-len(tokens))]\n",
        "print(\"Padded tokens are \\n {} \".format(padded_tokens))\n",
        "\n",
        "attn_mask=[ 1 if token != '[PAD]' else 0 for token in padded_tokens  ]\n",
        "print(\"Attention Mask are \\n {} \".format(attn_mask))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Padded tokens are \n",
            " ['[CLS]', 'i', 'love', 'mysore', 'and', 'i', 'leave', 'in', 'krishnarajanagar', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]'] \n",
            "Attention Mask are \n",
            " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQnthC8Mc4EE",
        "outputId": "37a8c515-4e17-41d1-e724-181fffb48e81"
      },
      "source": [
        "# step 4: Maintain a list of segment tokens:\n",
        "seg_ids=[0 for _ in range(len(padded_tokens))]\n",
        "\n",
        "print(\"Segment Tokens are \\n {}\".format(seg_ids))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Segment Tokens are \n",
            " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mZyUZdodAO9",
        "outputId": "b24b8fa8-aab2-4f1f-d01a-3e80de255bbf"
      },
      "source": [
        "# step 5 : Obtaining indices of the tokens in BERTâ€™s vocabulary:\n",
        "\n",
        "sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)\n",
        "print(\"senetence idexes \\n {} \".format(sent_ids))\n",
        "\n",
        "token_ids = torch.tensor(sent_ids).unsqueeze(0) \n",
        "attn_mask = torch.tensor(attn_mask).unsqueeze(0) \n",
        "seg_ids   = torch.tensor(seg_ids).unsqueeze(0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "senetence idexes \n",
            " [101, 1045, 2293, 20761, 1998, 1045, 2681, 1999, 100, 102, 0, 0, 0, 0, 0] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwdTjtrMdOxA",
        "outputId": "3000d961-8e8b-4f67-9eb8-7bf15ffeb3b4"
      },
      "source": [
        " model_output = model(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids)\n",
        "\n",
        " hidden_reps, cls_head = model_output[0],model_output[1]\n",
        "\n",
        "print(type(hidden_reps))\n",
        "print(hidden_reps.shape ) #hidden states of each token in inout sequence \n",
        "print(cls_head.shape ) #hidden states of each [cls]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paN3SkAffeG-"
      },
      "source": [
        "'''\n",
        "hidden_reps : # contextualized realationship between wprds in the sentence\n",
        "clas_head :   # Senetnce Representation\n",
        "\n",
        "hidden_rep.shape # torch.Size([1, 7, 768])\n",
        "\n",
        "Our batch size is 1, the sequence length is the token length, since we have 7 tokens, the sequence length is 7,\n",
        "The hidden size is the representation (embedding) size and it is 768 for the BERT-base model\n",
        "\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUPrL1xRfvB4"
      },
      "source": [
        "'''\n",
        "We can obtain the representation of each token as:\n",
        "\n",
        "hidden_rep[0][0] # gives the representation of the first token which is [CLS]\n",
        "hidden_rep[0][1] gives the representation of the second token which is 'I'\n",
        "hidden_repo[0][2] gives the representation of the third token which is 'love'\n",
        "\n",
        "In this way, we can obtain the contextual representation of all the tokens.\n",
        "\n",
        "This is basically the contextualized word embeddings of all the words in the given sentence.\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svjWTqw-fyfp"
      },
      "source": [
        "'''\n",
        "let's take a look at the cls_head. It contains the representation of the [CLS] token. \n",
        "\n",
        "clas_head.shape # [1, 768]\n",
        "\n",
        "cls_head holds the aggregate representation of the sentence, so we can use the \n",
        "cls_head as the representation of the given sentence 'I love Paris'.\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKaE3nnyfzL_"
      },
      "source": [
        "# Let's print the shape of cls_head :\n",
        "\n",
        "print(clas_head.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdHfpNS-f0pQ"
      },
      "source": [
        "'''\n",
        "We learned how to extract embeddings from the pre-trained BERT. B\n",
        "But these are the embeddings obtained only from the topmost encoder layer of BERT which is encoder 12.\n",
        "\n",
        " Can we also extract the embeddings from all the encoder layers of BERT? \n",
        "     - Yes\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}