{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06 Tokenization using the transformers Package.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jM6LeGMl-UH"
      },
      "source": [
        "# We can use the function encode_plus, which does the following in one go\n",
        "\n",
        "'''\n",
        "1. Tokenize the input sentence\n",
        "2. Add the [CLS] and [SEP] tokens.\n",
        "3. Pad or truncate the sentence to the maximum length allowed\n",
        "4. Encode the tokens into their corresponding IDs Pad or truncate all sentences to the same length.\n",
        "5. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzZfegCthtuv"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "import torch"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRCQjJM1hudP"
      },
      "source": [
        "# load the model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwkaBWJrhufy"
      },
      "source": [
        "sentence = \"I Love Mysore\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHBRofPVmWs-",
        "outputId": "f02735ef-9237-4b55-997f-3a537553a097"
      },
      "source": [
        "# Encode the sentence\n",
        "encoded = tokenizer.encode_plus(\n",
        "                                text=sentence,  # the sentence to be encoded\n",
        "                                add_special_tokens=True,  # Add [CLS] and [SEP]\n",
        "                                max_length = 64,  # maximum length of a sentence\n",
        "                                truncation=True,\n",
        "                                pad_to_max_length=True,  # Add [PAD]s\n",
        "                                return_attention_mask = True,  # Generate the attention mask\n",
        "                                return_tensors = 'pt',  # ask the function to return PyTorch tensors\n",
        "                              )\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uyxfcg4mWvN"
      },
      "source": [
        "# Get the input IDs and attention mask in tensor format\n",
        "input_ids = encoded['input_ids']\n",
        "attn_mask = encoded['attention_mask']"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls4MECMEmWxn",
        "outputId": "2ce70888-5909-468f-cbbc-4b49d1c6aea4"
      },
      "source": [
        "input_ids"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101,  1045,  2293, 20761,   102,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGjcvZDDmWz0",
        "outputId": "2beda4c0-5312-43ae-9492-11321efc686f"
      },
      "source": [
        "attn_mask"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd_IOOFImW2I"
      },
      "source": [
        "# The “attention mask” tells the model which tokens should be attended to and which (the [PAD] tokens) should not"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}