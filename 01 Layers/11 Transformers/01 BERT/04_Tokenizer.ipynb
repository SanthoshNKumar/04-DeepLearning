{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 Tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzZfegCthtuv"
      },
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRCQjJM1hudP",
        "outputId": "a63e3a06-97be-4fca-a8b9-6c5731e30833"
      },
      "source": [
        "# load the model\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwkaBWJrhufy",
        "outputId": "6f0ec063-c798-4515-eded-2a38c7ca296c"
      },
      "source": [
        "# case 1: all words or tokans are part of pretrained model vocabulary\n",
        "\n",
        "sentence = \"I Love Mysore\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'mysore']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iscwaMEkhuiL"
      },
      "source": [
        "# case 2: New token or not part of the pre trained model vocabulary will be tokenized based on below the strategy\n",
        "\n",
        "'''\n",
        "WordPiece Tokenizer\n",
        "\n",
        "    BERT uses special type of tokenizer called a word piece tokenizer.\n",
        "    The wordpiece tokeizer follows the subwords tokenization scheme\n",
        "    if any new word exist and not part of the pretained model.\n",
        "    \n",
        "Example : \"Let us start pretraining the model\"\n",
        "\n",
        "tokens = [let,us,start,pre,##train,##ing,the,model]\n",
        "\n",
        "here you can observe that the word 'Pretraining' is split into the following subwrods - pre,##train,##ing\n",
        "\n",
        "This is the effective way to handle OOV (Out of Vocabulary) words.\n",
        "\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrQYbBPehukg",
        "outputId": "084df803-5682-4200-d399-b05cd69a3b33"
      },
      "source": [
        "# case 2: \n",
        "\n",
        "sentence = \"I Love Mysore and i leave in Krishnarajanagar\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "# Krishnarajanagar is new word not part of pre traiend model vocabulary\n",
        "\n",
        "tokens"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'love',\n",
              " 'mysore',\n",
              " 'and',\n",
              " 'i',\n",
              " 'leave',\n",
              " 'in',\n",
              " 'krishna',\n",
              " '##raj',\n",
              " '##ana',\n",
              " '##gar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}