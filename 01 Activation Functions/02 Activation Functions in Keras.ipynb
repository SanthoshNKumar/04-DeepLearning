{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "\n",
    "\n",
    "Types of Activation Function\n",
    "\n",
    "    - Linear or Identity Activation Function\n",
    "    - Non-linear Activation Functions\n",
    "    \n",
    "\n",
    "'''\n",
    "    1. linear\n",
    "    2. sigmoid\n",
    "    3. Relu        : Rectified Linear Activation Function \n",
    "    4. Softmax\n",
    "    5. Softplus\n",
    "    6. Softsign\n",
    "    7. Tanh        : Hyperbolic tangent activation function\n",
    "    8. selu        : Scaled Exponential Linear Unit \n",
    "    9. elu         : Exponential Linear Unit\n",
    "    10.gelu        : Gaussian error linear unit (GELU)\n",
    "    11.hard_sigmoid \n",
    "    12.Exponential\n",
    "\n",
    "'''\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consuming or using Activation Functions in Moel Building\n",
    "\n",
    "# Applies an activation function to an output.\n",
    "\n",
    "#01 Add Activation Function\n",
    "model.add(layers.Dense(64, activation=activations.relu))\n",
    "\n",
    "#02 Like stacking layer\n",
    "model.add(layers.Dense(64))\n",
    "model.add(layers.Activation(activations.relu))\n",
    "\n",
    "#03  Using built-in activations may also be passed via their string identifier:\n",
    "model.add(layers.Dense(64, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Activation Functions\n",
    "\n",
    "# Binary Step :\n",
    "\n",
    "# If the input to the activation function is greater than a threshold, then the neuron is activated, else it is deactivated\n",
    "\n",
    "#f(x) = 1, x>=0\n",
    "      = 0, x<0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear\n",
    "# range(-infinity to infinity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.26894143, 0.5       , 0.7310586 , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid :  1 / (1 + exp(-x))\n",
    "# range : (0,1)\n",
    "\n",
    "a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)\n",
    "\n",
    "tf.keras.activations.sigmoid(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9950547, -0.7615942,  0.       ,  0.7615942,  0.9950547],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tanh : Hyperbolic tangent activation function\n",
    "# range(-1 to 1)\n",
    "# the tanh function is very similar to the sigmoid function.\n",
    "# The only difference is that it is symmetric around the origin.\n",
    "# If you compare it to sigmoid, it solves just one problem of being zero-centred\n",
    "\n",
    "a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n",
    "tf.keras.activations.tanh(a).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  5., 10.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : Degault if x<0 consider as value as 0\n",
    "# The main advantage of using the ReLU function over other activation functions is that it does not activate all the neurons at the same time.\n",
    "# This means that the neurons will only be deactivated if the output of the linear transformation is less than 0.\n",
    "\n",
    "# f(x) = max(0,x):\n",
    "\n",
    "foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32) \n",
    "# Default : tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)\n",
    "\n",
    "tf.keras.activations.relu(foo).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : alpha ; if value is lessthan zero means values/0.5\n",
    "\n",
    "tf.keras.activations.relu(foo, alpha=0.5).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 5., 5.], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : max_value\n",
    "tf.keras.activations.relu(foo, max_value=5).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0.,  0.,  0., 10.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : threshold\n",
    "tf.keras.activations.relu(foo, threshold=5).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=86, shape=(2, 4), dtype=float32, numpy=\n",
       "array([[0.       , 0.       , 3.8707087, 0.       ],\n",
       "       [0.       , 0.       , 4.2228813, 4.6672626]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "layer = layers.Dense(4, activation='relu')\n",
    "inputs = tf.convert_to_tensor([[-1,4,5,3],[1,6,0.3,1]])\n",
    "outputs = layer(inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELU :  Exponential Linear Unit \n",
    "\n",
    "# ELu is variant of Rectiufied Linear Unit (ReLU) that modifies the slope of the negative part of the function. \n",
    "\n",
    "# Unlike the leaky relu and parametric ReLU functions, instead of a straight line\n",
    "\n",
    "# The ELU is an excellent alternative to the ReLU â€“ it decreases bias shifts by pushing mean activation \n",
    "# towards zero during the training process.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# f(x) = x for x >= 0\n",
    "# f(x) =  alpha * (exp(x) - 1.) for x < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21194156, 0.57611688, 0.21194156])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax : \n",
    "# z = np.exp(x)\n",
    "# z_ = z/z.sum()\n",
    "\n",
    "\n",
    "# 1. The softmax is a more generalised form of the sigmoid\n",
    "# 2. softmax transforms the values between the range 0 and 1\n",
    "# 3. It is used in multi-class classification problems\n",
    "\n",
    "inp = np.asarray([1., 2., 1.])\n",
    "layer = tf.keras.layers.Softmax()\n",
    "layer(inp).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0611537e-09, 3.1326166e-01, 6.9314718e-01, 1.3132616e+00,\n",
       "       2.0000000e+01], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softplus : log(exp(x) + 1)\n",
    "a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)\n",
    "tf.keras.activations.softplus(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  0. ,  0.5], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softsign : x / (abs(x) + 1)\n",
    "\n",
    "# Similar to tanh functions\n",
    "# Tanh function that converges exponentially and The softsign function converges in a polynomial form\n",
    "\n",
    "a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)\n",
    "tf.keras.activations.softsign(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9950547, -0.7615942,  0.       ,  0.7615942,  0.9950547],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tanh : Hyperbolic tangent activation function\n",
    "# tanh : sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x))).\n",
    "# tanh : 2sigmoid(2x)-1\n",
    "\n",
    "# the tanh function is very similar to the sigmoid function.\n",
    "# The only difference is that it is symmetric around the origin.\n",
    "# The range of values in this case is from -1 to 1.\n",
    "\n",
    "# 1. If you compare it to sigmoid, it solves just one problem of being zero-centred\n",
    "\n",
    "a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n",
    "tf.keras.activations.tanh(a).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selu : Scaled Exponential Linear Unit \n",
    "# if x > 0: return scale * x\n",
    "# if x < 0: return scale * alpha * (exp(x) - 1)\n",
    "# constants (alpha=1.67326324 and scale=1.05070098)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gelu : Gaussian error linear unit (GELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04978707,  0.36787945,  1.        ,  2.7182817 , 20.085537  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exponential : exp(x).\n",
    "\n",
    "a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n",
    "\n",
    "tf.keras.activations.exponential(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swish:  f(x) = x*sigmoid(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
