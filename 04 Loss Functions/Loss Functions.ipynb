{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss : In machine learning, Loss function is used to find error or deviation in the learning process.\n",
    "\n",
    "# Keras provides quite a few loss function in the losses module and they are as follows\n",
    "\n",
    "#1. mean_squared_error\n",
    "#2. mean_absolute_error\n",
    "#3. mean_absolute_percentage_error\n",
    "#4. mean_squared_logarithmic_error\n",
    "#5. squared_hinge\n",
    "#6. hinge\n",
    "#7. categorical_hinge\n",
    "#8. logcosh\n",
    "#9. huber_loss\n",
    "#10. categorical_crossentropy\n",
    "#11. sparse_categorical_crossentropy\n",
    "#12. binary_crossentropy\n",
    "#13. kullback_leibler_divergence\n",
    "#14. poisson\n",
    "#15. cosine_proximity\n",
    "#16. is_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Probabilistic Losses\n",
    "    - Binary Crossentropy (BCE)\n",
    "    - Categorical Crossentropy or SoftMax Loss\n",
    "    - Sparse Categorical Crossentropy\n",
    "    - Poisson\n",
    "    - Kullback-Leibler Divergence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Regression losses\n",
    "    - Mean Squared error\n",
    "    - Mean Absolute Error\n",
    "    - Mean Absolute percentage error\n",
    "    - Mean Squared Logarithmic Error\n",
    "    - Cosine Similarity\n",
    "    - Huber\n",
    "    - Log Cosh\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hinge losses for “maximum-margin” classification\n",
    "    - Hinge\n",
    "    - Squared Hinge\n",
    "    - Categorical Hinge\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Hinge\n",
    "loss = maximum(1 - y_true * y_pred, 0)\n",
    "\n",
    "\n",
    "y_true values are expected to be -1 or 1\n",
    "If binary (0 or 1) labels are provided they will be converted to -1 or 1.\n",
    "\n",
    "'''\n",
    "\n",
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "\n",
    "h = tf.keras.losses.Hinge()\n",
    "    \n",
    "h(y_true,y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.86"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "SquaredHinge class\n",
    "\n",
    "loss = square(maximum(1 - y_true * y_pred, 0))\n",
    "\n",
    "y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1\n",
    "\n",
    "'''\n",
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "\n",
    "h = tf.keras.losses.SquaredHinge()\n",
    "\n",
    "h(y_true,y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling with 'sample_weight'\n",
    "h(y_true, y_pred, sample_weight=[1, 0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.72"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'sum' reduction type.  \n",
    "h = tf.keras.losses.SquaredHinge(reduction=\"sum\")\n",
    "\n",
    "h(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.46, 2.26], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using 'None' reduction type.  \n",
    "h = tf.keras.losses.SquaredHinge(reduction=\"none\")\n",
    "\n",
    "h(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4000001"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "CategoricalHinge class\n",
    "\n",
    "loss = maximum(neg - pos + 1, 0) where neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)\n",
    "\n",
    "pos = np.sum(y_true * y_pred, axis=-1)\n",
    "neg = np.amax((1. - y_true) * y_pred, axis=-1)\n",
    "loss = np.maximum(0., neg - pos + 1.)\n",
    "\n",
    "\n",
    "y_true values are expected to be 0 or 1.\n",
    "\n",
    "'''\n",
    "\n",
    "y_true = [[0, 1], [0, 0]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "\n",
    "h = tf.keras.losses.CategoricalHinge()\n",
    "h(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Each observation is weighted by the fraction of the class it belongs to (reversed) so that the loss for minority \n",
    "class observations is more important when calculating the loss.  \n",
    "\n",
    "weights = { 0:1.01300017,1:0.88994364,2:1.00704935, 3:0.97863318,4:1.02704553, 5:1.10680686,6:1.01385603,7:0.95770152, \n",
    "            8:1.02546573,9:1.00857287}\n",
    "            \n",
    "model.fit(x_train, y_train,verbose=1, epochs=10,class_weight=weights)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# You can also pass weights at the compile stage.\n",
    "weights = [1.013, 0.889, 1.007, 0.978, 1.027,1.106,1.013,0.957,1.025, 1.008]\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              loss_weights=weights,\n",
    "              metrics=['accuracy']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions are passed during the compile stage\n",
    "\n",
    "loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss_function, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error : 01\n",
    "\n",
    "# MSE = np.square(np.subtract(Y_true,Y_pred)).mean()\n",
    "\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "y_true = [[45., 40.], \n",
    "          [0., 0.]]\n",
    "y_pred = [[12., 4.], \n",
    "          [1., 0.]]\n",
    "\n",
    "# How it is working\n",
    "\n",
    "'''\n",
    "Step 1:  Find the First batch value and get MSE \n",
    "\n",
    "y_true = [45., 40.]\n",
    "y_pred = [12., 4.]\n",
    "\n",
    "np.sum([np.square((x-y)) for x,y in zip(y_true, y_pred)]) / len(y_true) = 1192.5\n",
    "\n",
    "Step 2:  Find the Second batch value and get MSE \n",
    "\n",
    "y_true = [45., 40.]\n",
    "y_pred = [12., 4.]\n",
    "\n",
    "np.sum([np.square((x-y)) for x,y in zip(y_true, y_pred)]) / len(y_true) = 0.5\n",
    "\n",
    "Step 3:  Find Average value of both : (1192.5 + 0.5) / 2 = 596.5\n",
    "\n",
    "'''\n",
    "\n",
    "mse(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error : 02\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0121489"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Mean Squared Logarithmic Error : (MSLE)\n",
    "\n",
    "MSE = np.square(np.subtract(np.log(Y_true +1),np.log(Y_pred +1))).mean()\n",
    "\n",
    "Find the relative difference between the true and the predicted value\n",
    "\n",
    "'''\n",
    "\n",
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [1., 0.]]\n",
    "\n",
    "y_true = [45., 40]\n",
    "y_pred = [12., 4]\n",
    "msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
    "\n",
    "'''\n",
    "msle = np.sum([np.square((np.log(x +1)-np.log(y+1))) for x,y in zip(y_true, y_pred)]) / len(y_true)\n",
    "\n",
    "'''\n",
    "msle(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.5"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Mean Absolute Error :MAE\n",
    "\n",
    "MAE = np.abs(np.subtract(Y_true,Y_pred)).mean()\n",
    "\n",
    "Cosider only the magniture (length : sqrt.(x2 + y2) )\n",
    "does not conisder direction (Negative or Positisve)\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "y_true = [[0., 1.], \n",
    "          [0., 0.]]\n",
    "\n",
    "y_pred = [[1., 1.], \n",
    "          [1., 0.]]\n",
    "          \n",
    "'''\n",
    "\n",
    "y_true = [45., 40]\n",
    "y_pred = [12., 4]\n",
    "\n",
    "\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "'''np.sum([np.abs(x-y) for x,y in zip(y_true,y_pred)])/2'''\n",
    "\n",
    "mae(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.666664"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Mean Absolute percentage error\n",
    "\n",
    "MAPE = (np.abs(np.subtract(Y_true,Y_pred)) / Y_true).mean() * 100\n",
    "\n",
    "Cosider only the magniture (length : sqrt.(x2 + y2) )\n",
    "does not conisder direction (Negative or Positisve)\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "y_true = [[0., 1.], \n",
    "          [0., 0.]]\n",
    "\n",
    "y_pred = [[1., 1.], \n",
    "          [1., 0.]] \n",
    "'''\n",
    "\n",
    "y_true = [45., 40]\n",
    "y_pred = [12., 4]\n",
    "\n",
    "mape = tf.keras.losses.mean_absolute_percentage_error\n",
    "\n",
    "'''(np.sum([(np.abs(x-y)/x) for x,y in zip(y_true,y_pred)])/2) * 100'''\n",
    "\n",
    "mape(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67082036"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cosine Similarity\n",
    "# Measure of similarity between two non-zero vectors of an inner product space\n",
    "# cosine similarity (CS) = (A . B) / (||A|| ||B||)\n",
    "\n",
    "'''\n",
    "# got product of vector A and Vector B\n",
    "dot_ab = np.sum([(num1*num2) for num1,num2 in zip(y_true,y_pred)])\n",
    "\n",
    "# Magniture of vector A\n",
    "mag_a = np.sqrt(np.sum([num1*num1 for num1 in y_true]))\n",
    "\n",
    "# Magniture of vector A\n",
    "mag_b = np.sqrt(np.sum([num2*num2 for num2 in y_pred]))\n",
    "\n",
    "cosime_similarity  = dot_ab/ (mag_a*mag_b) = 0.7999999999999998\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "y_true = [[0., 1.], [1., 1.]]\n",
    "y_pred = [[1., 0.], [1., 1.]]\n",
    "\n",
    "'''\n",
    "y_true = [1., 1., 1., 1., 0.,0.]\n",
    "y_pred = [0., 1., 1., 1., 1., 1.]\n",
    "\n",
    "\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity()\n",
    "\n",
    "cosine_loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9482092"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Huber :(MSE + MAE) based on delta value we chosse either MSE or MAE\n",
    "\n",
    "This function is quadratic(MSE) for small values of a and linear(MAE) for large values, \n",
    "\n",
    "loss = 0.5 * x^2                  if |x| <= d\n",
    "loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d\n",
    "\n",
    "where d is delta\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "y_true = [24.0,50.0,15.0,38.0,87.0]\n",
    "y_pred = [21.54,47.46,17.21,36.58,87.28]\n",
    "delta =4\n",
    "\n",
    "# distance between Actual and true value is less than 4 : So delta :4\n",
    "# [np.subtract(x,y) for x,y in zip(y_true,y_actual)]\n",
    "\n",
    "tota_error =0\n",
    "tota_points = 0\n",
    "\n",
    "for index,value in enumerate(y_true):\n",
    "    tota_points +=1\n",
    "    error = y_true[index] - y_pred[index]\n",
    "    \n",
    "    if error <= delta:\n",
    "        huber_error = 0.5 * round((error*error)/2,2)\n",
    "    else:\n",
    "        #huber_error = round(delta*error) / (0.5 * (delta*delta))\n",
    "        huber_error = 0.5 * (delta*delta) + delta * (np.abs(error) - delta)\n",
    "    tota_error += huber_error\n",
    "\n",
    "huber = round(tota_error/tota_points,2)\n",
    "\n",
    "'''\n",
    "\n",
    "y_true = [[0, 1], [0, 0]]\n",
    "y_pred = [[0.5, 0.4], [0.4, 0.5]]\n",
    "\n",
    "y_true = [24.0,50.0,15.0,38.0,87.0]\n",
    "y_pred = [21.54,47.46,17.21,36.58,87.28]\n",
    "\n",
    "hub_loss = tf.keras.losses.Huber(delta=4.0) # default delta =1\n",
    "hub_loss(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.195665"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Log Cosh : Computes the logarithm of the hyperbolic cosine of the prediction error\n",
    "Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "logcosh = log((exp(x) + exp(-x))/2)\n",
    "\n",
    "x is the error y_pred - y_true.\n",
    "\n",
    "'''\n",
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[1., 1.], [0., 0.]]\n",
    "\n",
    "\n",
    "y_true = [24.0,50.0,15.0,38.0,87.0]\n",
    "y_pred = [21.54,47.46,17.21,36.58,87.28]\n",
    "\n",
    "l = tf.keras.losses.LogCosh()\n",
    "\n",
    "l(y_true, y_pred).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hinge Loss Function\n",
    "\n",
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[0.5, 0.4], [0.4, 0.5]]\n",
    "\n",
    "y_true = [24.0,50.0,15.0,38.0,87.0]\n",
    "y_pred = [21.54,47.46,17.21,36.58,87.28]\n",
    "\n",
    "h_loss = tf.keras.losses.Hinge()\n",
    "h_loss(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Squared Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24691972"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "# Binary Cross Entropy : \n",
    "    - used where two class present in the traget varible\n",
    "    - By default, the sum_over_batch_size reduction is used.\n",
    "    - This means that the loss will return the average of the per-sample losses in the batch.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3]\n",
    "\n",
    "# The prediction is a probability vector, meaning it represents predicted probabilities of all classes, summing up to 1.\n",
    "\n",
    "'''\n",
    "result = []\n",
    "\n",
    "for i in range(len(actual)):\n",
    "    p = y_true[i]\n",
    "    q = y_pred[i]\n",
    "    \n",
    "    ce = - p * np.log(q) - (1-p)* np.log(1-q)\n",
    "    \n",
    "    result.append(ce)\n",
    "    print(\"y={0} yhat={1} ce:{2}\".format(p,q,ce))\n",
    "    \n",
    "    \n",
    "print(\"Average Cross Entropy:{0}\".format(np.mean(result)))\n",
    "\n",
    "'''\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "bce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40036726"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true= [0,0,0,1]\n",
    "y_pred = [0.1,0.3,0.2,0.4]\n",
    "\n",
    "\n",
    "cce = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4003674356962309"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(len(y_true)):\n",
    "    p = y_true[i]\n",
    "    q = y_pred[i]\n",
    "    \n",
    "    ce = - p * np.log(q) - (1-p)* np.log(1-q)\n",
    "    \n",
    "    result.append(ce)\n",
    "    #print(\"y={0} yhat={1} ce:{2}\".format(p,q,ce))\n",
    "    \n",
    "np.mean(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a good cross-entropy score?\n",
    "\n",
    "'''\n",
    "Cross-Entropy = 0.00: Perfect probabilities.\n",
    "Cross-Entropy < 0.02: Great probabilities.\n",
    "Cross-Entropy < 0.05: On the right track.\n",
    "Cross-Entropy < 0.20: Fine.\n",
    "Cross-Entropy > 0.30: Not great.\n",
    "Cross-Entropy > 1.00: Terrible.\n",
    "Cross-Entropy > 2.00 Something is broken.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Log Loss: 0.400\n"
     ]
    }
   ],
   "source": [
    "# Log Loss : “log loss“,or  “cross-entropy” or “negative log-likelihood” a\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from numpy import asarray\n",
    "\n",
    "# define data as expected, e.g. probability for each event {0, 1}\n",
    "\n",
    "# calculate the average log loss\n",
    "\n",
    "ll = log_loss(y_true, y_pred)\n",
    "print('Average Log Loss: %.3f' % ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Softmax function\"\"\"\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9162907"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Categorical Cross Entropy\n",
    "\n",
    "#- It compares the predicted probability distribution with target probability distribution\n",
    "\n",
    "#y_true = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "#y_pred = [0.8, 0.9, 0.9, 0.6, 0.8, 0.1, 0.4, 0.2, 0.1, 0.3] \n",
    "\n",
    "y_true= [0,0,0,1]\n",
    "y_pred = [0.1,0.3,0.2,0.4]  # predicted probability value\n",
    "\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.916290731874155"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CCE manualy\n",
    "\n",
    "np.sum(-(y_true * np.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'convert_to_tensort'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5ef7724e0c59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.95\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\insakum46\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\util\\module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m       \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'convert_to_tensort'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "y_true = [1., 2.]\n",
    "y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy(tf.convert_to_tensort(y_true), tf.convert_to_tensor(y_pred))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\insakum46\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (2,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-98cd8b426430>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (2,3) "
     ]
    }
   ],
   "source": [
    "np.sum(-(y_true * np.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse Categorical Crossentropy\n",
    "\n",
    "\n",
    "#  if you use categorical_crossentropy you use one hot encoding, \n",
    "#  if you use sparse_categorical_crossentropy you encode as normal integers. \n",
    "\n",
    "\n",
    "# When to use Categorical Cross Entropy\n",
    "\n",
    "# If your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings:\n",
    "\n",
    "# If your targets are integers, use sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3798114"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Categorical Cross Entroy : One One Encoded of y_true \n",
    "\n",
    "y_true = [[0, 0, 1],\n",
    "          [1, 0, 0],\n",
    "          [0, 0, 1]]\n",
    "\n",
    "y_pred = [[0.1, 0.1, 0.8],\n",
    "          [0.5, 0.2, 0.3],\n",
    "          [0.0, 0.2, 0.8]]\n",
    "\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.22314355, 0.6931472 , 0.22314365], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparse_categorical_crossentropy\n",
    "\n",
    "y_true = [[2, 0, 2]]\n",
    "\n",
    "y_pred = [[0.1, 0.1, 0.8],\n",
    "          [0.5, 0.2, 0.3],\n",
    "          [0.0, 0.2, 0.8]]\n",
    "\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy(tf.convert_to_tensor(y_true),tf.convert_to_tensor(y_pred))\n",
    "\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=138, shape=(2,), dtype=float64, numpy=array([1.32565812, 1.1877869 ])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Poisson\n",
    "# Poisson = y_pred - y_true * log(y_pred)\n",
    "\n",
    "y_true = np.random.randint(0, 2, size=(2, 3))\n",
    "y_pred = np.random.random(size=(2, 3))\n",
    "\n",
    "loss = tf.keras.losses.poisson(y_true, y_pred)\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45814306"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kullback-Leibler Divergence\n",
    "\n",
    "# loss = y_true * log(y_true / y_pred)\n",
    "\n",
    "y_true = [[0, 1], [0, 0]]\n",
    "\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "\n",
    "kl = tf.keras.losses.KLDivergence()\n",
    "kl(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[pi * np.log2(qi/pi) for pi, qi in zip(y_true,y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4595, shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0.21268466, 1.170648  , 4.129609  , 3.2590992 , 4.58801   ],\n",
       "       [0.8579177 , 0.6435966 , 4.2675934 , 2.466893  , 0.00870855],\n",
       "       [0.28124034, 0.18294993, 0.04965096, 0.11762683, 2.1920042 ],\n",
       "       [1.1066352 , 0.64277405, 0.00677719, 3.7263577 , 0.02155003],\n",
       "       [0.4258032 , 1.147773  , 0.03896642, 0.059942  , 0.00458926]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multi label classification Loss Function\n",
    "\n",
    "logits = tf.Variable(np.array([[ 1.4397182 , -0.7993438 ,  4.113389  ,  3.2199187 ,  4.5777845 ],\n",
    "                               [ 0.30619335,  0.10168511,  4.253479  ,  2.3782277 ,  4.7390924 ],\n",
    "                               [ 1.124632  ,  1.6056736 ,  2.9778094 ,  2.0808482 ,  2.0735667 ],\n",
    "                               [ 0.7051575 , -0.10341895,  4.990803  ,  3.7019827 ,  3.8265839 ],\n",
    "                               [ 0.6333333 , -0.76601076,  3.2255085 ,  2.7842572 ,  5.3817415 ]]),dtype = tf.float32)\n",
    "\n",
    "targets = tf.Variable(np.array([[1, 1, 0, 0, 0],\n",
    "                                [0, 1, 0, 0, 1],\n",
    "                                [1, 1, 1, 1, 0],\n",
    "                                [0, 0, 1, 0, 1],\n",
    "                                [1, 1, 1, 1, 1]]),dtype = tf.float32)\n",
    "\n",
    "\n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets )\n",
    "\n",
    "cross_entropy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
