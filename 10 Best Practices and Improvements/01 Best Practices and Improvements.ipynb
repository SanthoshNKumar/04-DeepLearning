{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Best Practices\n",
    "\n",
    "1. Never initialize weights from 0 or with same values as then there won’t be update.\n",
    "   this is known as symmetry breaking problem.\n",
    "   \n",
    "2. Always normalize the inputs.\n",
    "\n",
    "3. Use ReLU in hidden layer activation, but be careful with the learning rate and monitor the fraction of dead units\n",
    "\n",
    "4. If ReLU is giving problems. Try Leaky ReLU, PReLU, Maxout. Do not use sigmoid\n",
    "\n",
    "5. Never initialize the weights to large value otherwise model will blow up.\n",
    "\n",
    "6. Weights initialized must be inversely proportion to input.\n",
    "\n",
    "7. The sigmoid and hyperbolic tangent activation functions cannot be used in networks with many layers due to \n",
    "   the vanishing gradient problem\n",
    "   \n",
    "8. Use He initialization for ReLu and Leaky ReLu.\n",
    "\n",
    "9. Rectified Linear Unit (ReLU) activation function can help you in preventing vanishing gradients.\n",
    "\n",
    "10. The problem of vanishing gradient requires to use of small learning rates with gradient descent.\n",
    "    so the model will get some more time to converge\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Best practices:\n",
    "\n",
    "1. Use Batch nromalization can help you in preventing exploding gradients.\n",
    "    - standard deviation of one and mean output activation of zero.\n",
    "\n",
    "2. Use Gradient Clipping:\n",
    "    - Gradient clipping involves forcing the gradient values (element-wise) to a specific minimum or maximum value \n",
    "      if the gradient exceeded an expected range.\n",
    "\n",
    "3. Use callback help to build better models.\n",
    "\n",
    "4. The learning rate should not be too low because the training progresses very slowly.\n",
    "   and it will take many weights updates before it comes to the minimum point\n",
    "   \n",
    "5. The learning rate should not be too high because the training progresses very rapidly\n",
    "   and it will often fail to converge or even diverge.\n",
    "   \n",
    "6. If you want to train the neural network in less time and more efficiently, then Adam is the best optimizer.\n",
    "\n",
    "7. Use Checkpoints\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recommendation on Activation Functions\n",
    "\n",
    "    - Softmax is used only for the output layer\n",
    "    - Sigmoid and tanh functions are sometimes avoided due to the vanishing gradient problem\n",
    "    - Tanh is avoided most of the time due to dead neuron problem\n",
    "    - ReLU function should only be used in the hidden layers\n",
    "    - An output layer can be linear activation function in case of regression problems.\n",
    "    - Multilayer Perceptron (MLP): ReLU activation function.\n",
    "    - Convolutional Neural Network (CNN): ReLU activation function.\n",
    "    - Recurrent Neural Network: Tanh and/or Sigmoid activation function.\n",
    "    - As a rule of thumb, you can begin with using ReLU function and then move over to other activation functions \n",
    "      in case ReLU doesn’t provide with optimum results\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Cecklist to improve performance:\n",
    "\n",
    "1. Analyze errors (bad predictions) in the validation dataset.\n",
    "\n",
    "2. Monitor the activations. Consider batch or layer normalization if it is not zero centered or Normal distributed.\n",
    "\n",
    "3. Monitor the percentage of dead nodes.\n",
    "\n",
    "4. Apply gradient clipping (in particular NLP) to control exploding gradients.\n",
    "\n",
    "5. Shuffle dataset (manually or programmatically).\n",
    "\n",
    "6. Balance the dataset (Each class has the similar amount of samples).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tuning\n",
    "    - Learning rate tuning\n",
    "    - Mini-batch size\n",
    "    - Regularization factors\n",
    "    - Layer-specific hyperparameters (like dropout)\n",
    "'''\n",
    "\n",
    "'''\n",
    "Advanced Tuning\n",
    "    - Learning rate decay schedule\n",
    "    - Momentum\n",
    "    - Early stopping\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Hyerparameters in Deep Learning :\n",
    "  - Number of Hidden units\n",
    "  - Learning rate\n",
    "  - Epochs\n",
    "  - Network weight Initialization\n",
    "  - Activation Function\n",
    "  - Batch Size\n",
    "  - Momentum\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Challenges:\n",
    "\n",
    "    1. Presence of Data Available for Training our Model\n",
    "    2. Model Overfitting\n",
    "    3. Model Underfitting\n",
    "    4. Training Time is too High\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Challenges with all types Gradient Descent:\n",
    "    - Choosing an optimum value of the learning rate. \n",
    "    - If the learning rate is too small than gradient descent may take ages to converge.\n",
    "    - Having a constant learning rate for all the parameters. There may be some parameters which we may not want to \n",
    "      change at the same rate.May get trapped at local minima.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Improving The Keras Model \n",
    "    - Add additional layers to our network(Hidden layers with neurons)\n",
    "    - Improving the simple net in Keras with dropout\n",
    "    - Testing different optimizers in Keras\n",
    "    - Tune the number of neurons in hidden layers, etc.\n",
    "    - Add or reduce the number of dense layers\n",
    "    - Increasing the number of epochs\n",
    "    - Controlling the optimizer learning rate\n",
    "    - Increasing the number of internal hidden neurons\n",
    "    - Increasing the size of batch computation(Batch Size)\n",
    "    - Adopting regularization for avoiding overfitting\n",
    "    - Hyperparameters tuning\n",
    "        - hidden neurons, BATCH_SIZE,epochs,\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Improving Simple Keras Network :\n",
    "    - By adding additional layers (Hidden)\n",
    "    - Randomly drop with the dropout probability some of the values propagated\n",
    "    - Try different Optimizers  (RMSProp,Adam,SGD)\n",
    "    - Increasing number of epochs(20 to 200) : Increasing  the value by 10\n",
    "    - Controlling Optimizer learning rate[0.1,0.01,0.001,0.001,0.0001,0.0001] = [1E-0, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, 1E-7]\n",
    "    - Increasing the number of internal hidden neurons [32,64,128,255,512,1024]\n",
    "    - Increasing the size of batch computation[64,128,256,512]\n",
    "    - try Different Types of regularization(L1 regularization;L2 regularization; Elastic net regularization:)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Optimization :\n",
    "\n",
    "    - Balance your data set\n",
    "        - Subsample Majority Class: You can balance the class distributions by subsampling the majority class.\n",
    "        - Oversample Minority Class: Sampling with replacement can be used to increase your minority class proportion.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note :\n",
    "\n",
    "    - Poor choice of learning rate that results in large weight updates.\n",
    "    - Poor choice of data preparation, allowing large differences in the target variable.\n",
    "    - Poor choice of loss function, allowing the calculation of large error values.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "When to use : Optimizers\n",
    "\n",
    "    - Mini Batch SGD           --> Use when network is small or shallow\n",
    "    - Momentum with GD/ SGD    --> Works well most of the cases but slighly slower in converges\n",
    "    - AdaGrad/AdaDelte/RMSProp --> Use when there is sparse data\n",
    "    - Adam and Its Variation   --> Always Recommended\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Consider while Weight Initialization :\n",
    "    - Initializing all weights to 0:\n",
    "            - Initializing all the weights with zeros leads the neurons to learn the same features during training.\n",
    "            - It acts as linear model\n",
    "    - Initializing weights randomly :\n",
    "            - Using standard normal distribution\n",
    "                - leads to two issue\n",
    "                    - Vanishing gradients\n",
    "                    - Exploding gradients\n",
    "    - Initilization weights too large \n",
    "        - A  too-large initialization leads to exploding gradients\n",
    "\n",
    "    - Initilization weights too small:\n",
    "        - A too-small initialization leads to vanishing gradients\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "How to find appropriate initialization values:\n",
    "    - The mean of the activations should be zero.\n",
    "    - The variance of the activations should stay the same across every layer.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "'''\n",
    "When to Use it?\n",
    "\n",
    "\n",
    "Accuracy :\n",
    "\n",
    "- When to use :  classification problems which are well balanced and not skewed or No class imbalance\n",
    "- When not to use : target class of dataset is imbalanced (target variable classes in the data are a majority of one class)\n",
    "\n",
    "\n",
    "Sensitivity / Specificity\n",
    "- There are some cases where Sensitivity is important and need to be near to 1.\n",
    "- There are business cases where Specificity is important and need to be near to 1.\n",
    "- We need to understand the business problem and decide the importance of Sensitivity and Specificity.\n",
    "\n",
    "\n",
    "- Precision is a useful metric in cases where False Positive is a higher concern than False Negatives.\n",
    "  \n",
    "- Recall is important in medical cases where it doesn’t matter whether we raise a false alarm but \n",
    "  the actual positive cases should not go undetected.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "When to use Metrics (MAE,MSE and RMSE)\n",
    "\n",
    "\n",
    "When Outliers in our data are while creating model. Do we include the outliers in our model creation or do we ignore them?\n",
    "    - Accepting the outliers as part of model devlopment in this case. I would want to use the MSE to ensure that\n",
    "      my model takes these outliers into account more.\n",
    "    - If I wanted to downplay their significance, I would use the MAE since the outlier residuals won’t contribute \n",
    "      as much to the total error as MSE.\n",
    "      \n",
    "MAE:  \n",
    "    - If there are many outliers then you may consider using Mean Absolute Error. it doesn't punish huge errors.\n",
    "    - If we want to treat all errors equally,  MAE is a better measure.\n",
    "\n",
    "\n",
    "MSE : \n",
    "    - It used when mostly when the dataset contains outliers, or unexpected values (too high or too low values) and we can \n",
    "      address these issue by reducing the MSE.\n",
    "    -  If we want to give more weight-age to large errors, MSE/RMSE is better\n",
    "      \n",
    "      \n",
    "RMSE : \n",
    "       - This basically implies that RMSE assigns a higher weight to larger errors. \n",
    "       \n",
    "       - This indicates that RMSE is much more useful when large errors are present and they drastically affect the \n",
    "         model's performance\n",
    "       - when your observations(Target Value) conditional distribution is asymmetric and you want an unbiased fit.\n",
    "       \n",
    "- If you have outlier in the data and you want to ignore them, MAE is a better option but if you want to account for them \n",
    "  in your loss function, go for MSE/RMSE.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
