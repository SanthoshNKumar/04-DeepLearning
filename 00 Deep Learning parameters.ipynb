{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers API overview\n",
    "\n",
    "'''\n",
    "Layer activations\n",
    "    1. relu function : Rectified Linear Activation Function \n",
    "    2. Sigmoid \n",
    "    3. Softmax\n",
    "    4. Softplus\n",
    "    5. Softsign\n",
    "    6. Tanh : Hyperbolic tangent activation function\n",
    "    7. selu : Scaled Exponential Linear Unit \n",
    "    8. elu  : Exponential Linear Unit\n",
    "    9. gelu : Gaussian error linear unit (GELU)\n",
    "    10.hard_sigmoid\n",
    "    11.linear\n",
    "    13.Exponential\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Layer Weight Initilizer\n",
    "    1. RandomNormal class\n",
    "    2. RandomUniform class\n",
    "    3. TruncatedNormal class\n",
    "    4. Zeros class\n",
    "    5. Ones class\n",
    "    6. GlorotNormal class\n",
    "    7. GlorotUniform class\n",
    "    8. Identity class\n",
    "    9. Orthogonal class\n",
    "    10. Constant class\n",
    "    11. VarianceScaling class\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Layer weight regularizers\n",
    "    1. l1 class\n",
    "    2. l2 class\n",
    "    3. l1_l2 class\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Layer weight constraints\n",
    "    1. MaxNorm class\n",
    "    2. MinMaxNorm class\n",
    "    3. NonNeg class\n",
    "    4. UnitNorm class\n",
    "    5. RadialConstraint class\n",
    "    \n",
    "'''\n",
    "# Constraints module allow setting constraints(ex>Non-Negativity) on model parameters during traing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Core layers\n",
    "    1. Input object\n",
    "    2. Dense layer\n",
    "    3. Activation layer\n",
    "    4. Embedding layer\n",
    "    5. Masking layer\n",
    "    6. Lambda layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convolution layers\n",
    "    1.Conv1D layer\n",
    "    2.Conv2D layer\n",
    "    3.Conv3D layer\n",
    "    4.SeparableConv1D layer\n",
    "    5.SeparableConv2D layer\n",
    "    6.DepthwiseConv2D layer\n",
    "    7.Conv2DTranspose layer\n",
    "    8.Conv3DTranspose layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Pooling layers\n",
    "    1. MaxPooling1D layer\n",
    "    2. MaxPooling2D layer\n",
    "    3. MaxPooling3D layer\n",
    "    4. AveragePooling1D layer\n",
    "    5. AveragePooling2D layer\n",
    "    6. AveragePooling3D layer\n",
    "    7. GlobalMaxPooling1D layer\n",
    "    8. GlobalMaxPooling2D layer\n",
    "    9. GlobalMaxPooling3D layer\n",
    "    10. GlobalAveragePooling1D layer\n",
    "    11. GlobalAveragePooling2D layer\n",
    "    12. GlobalAveragePooling3D layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Recurrent layers\n",
    "\n",
    "    1. LSTM layer\n",
    "    2. GRU layer\n",
    "    3. SimpleRNN layer\n",
    "    4. TimeDistributed layer\n",
    "    5. Bidirectional layer\n",
    "    6. ConvLSTM2D layer\n",
    "    7. Base RNN layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Normalization layers\n",
    "    1. BatchNormalization layer\n",
    "    2. LayerNormalization layer\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Regularization layers\n",
    "    1. Dropout layer\n",
    "    2. SpatialDropout1D layer\n",
    "    3. SpatialDropout2D layer\n",
    "    4. SpatialDropout3D layer\n",
    "    5. GaussianDropout layer\n",
    "    6. GaussianNoise layer\n",
    "    7. ActivityRegularization layer\n",
    "    8.AlphaDropout layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Reshaping layers\n",
    "    1. Reshape layer\n",
    "    2. Flatten layer\n",
    "    3. RepeatVector layer\n",
    "    4. Permute layer\n",
    "    5. Cropping1D layer\n",
    "    6. Cropping2D layer\n",
    "    7. Cropping3D layer\n",
    "    8. UpSampling1D layer\n",
    "    9. UpSampling2D layer\n",
    "    10.UpSampling3D layer\n",
    "    11. ZeroPadding1D layer\n",
    "    12. ZeroPadding2D layer\n",
    "    13.ZeroPadding3D layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Merging layers\n",
    "    1. Concatenate layer\n",
    "    2. Average layer\n",
    "    3. Maximum layer\n",
    "    4. Minimum layer\n",
    "    5. Add layer\n",
    "    6. Subtract layer\n",
    "    8. Multiply layer\n",
    "    9. Dot layer\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Locally-connected layers\n",
    "    1. LocallyConnected1D layer\n",
    "    2. LocallyConnected2D layer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Optimizers \n",
    "    1. Adadelta : Extension of Adagrad : Adadelta(adaptive delta)\n",
    "    2. Adagrad  : Adaptive Gradient Algorithm\n",
    "    3. Adam     : Adaptive Moment Estimation\n",
    "    4. Adamax   :\n",
    "    5. Ftrl     : Follow The Regularized Leader\n",
    "    6. Nadam    : Nesterov-accelerated Adaptive Moment Estimation\n",
    "    7. RMSprop  : Root Mean Square Propagation \n",
    "    8. SGD      : Stochastic Gradient Descent\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss Functions\n",
    "    1. mean_squared_error\n",
    "    2. mean_absolute_error\n",
    "    3. mean_absolute_percentage_error\n",
    "    4. mean_squared_logarithmic_error\n",
    "    5. squared_hinge\n",
    "    6. hinge\n",
    "    7. categorical_hinge\n",
    "    8. logcosh\n",
    "    9. huber_loss\n",
    "    10. categorical_crossentropy\n",
    "    11. sparse_categorical_crossentropy\n",
    "    12. binary_crossentropy\n",
    "    13. kullback_leibler_divergence\n",
    "    14. poisson\n",
    "    15. cosine_proximity\n",
    "    16. is_categorical_crossentropy\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Metrics\n",
    "    1. AUC : Area Under Curve\n",
    "    2. Accuracy\n",
    "    3. Binary Accuracy\n",
    "    4. BinaryCrossEntropy\n",
    "    5. CategoricalAccuracy\n",
    "    6. CategoricalCrossEntropy\n",
    "    7. CategoricalHinge\n",
    "    8. CosineSimilarity\n",
    "    9. FalseNegative\n",
    "    10. FalsePositive\n",
    "    11. Hinge\n",
    "    12. KLDivergence\n",
    "    13. LogCoshError\n",
    "    14. Mean\n",
    "    15. MeanAbolsuteError\n",
    "    16. MeanAbsolutePercentageError\n",
    "    17. MeanIou\n",
    "    18. MeanRelatievError\n",
    "    19. MeanSqaredError\n",
    "    20. MeanSquaredLogarithmicError\n",
    "    21. MeanTensor\n",
    "    22. Poission\n",
    "    23. PrecisionAtRecall\n",
    "    24. Recall\n",
    "    25. RecallAtPrecision\n",
    "    26. RootMeanSquaredError\n",
    "    27. SensitivityAtSpecificity\n",
    "    28. SparseCatgoricalAccuracy\n",
    "    29. SparseTopKCategoricalAccuracy\n",
    "    30.SpecificityAtSensivity\n",
    "    31.SquardHinge\n",
    "    32. Sum\n",
    "    33. TopKCategoricalAccuracy\n",
    "    34. TrueNegative\n",
    "    35 TruePositives\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_3:0\", shape=(?, 3), dtype=float32)\n",
      "Tensor(\"input_4:0\", shape=(?, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Input : input is used to instantiate a Keras tensor.\n",
    "\n",
    "from tensorflow.keras import Input\n",
    "\n",
    "inputs = Input(shape=(3,))\n",
    "print(inputs) # 2D\n",
    "\n",
    "inputs = Input(shape=(3,4))\n",
    "print(inputs) # 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****Activation Function*********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.26894143, 0.5       , 0.7310586 , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sigmoid :  1 / (1 + exp(-x))\n",
    "\n",
    "a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)\n",
    "\n",
    "tf.keras.activations.sigmoid(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  5., 10.], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : Degault if x<0 consider as value as 0\n",
    "\n",
    "foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32) \n",
    "# Default : tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0)\n",
    "\n",
    "tf.keras.activations.relu(foo).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : alpha ; if value is lessthan zero means values/0.5\n",
    "\n",
    "tf.keras.activations.relu(foo, alpha=0.5).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 5., 5.], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : max_value\n",
    "tf.keras.activations.relu(foo, max_value=5).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0.,  0.,  0., 10.], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu function : threshold\n",
    "tf.keras.activations.relu(foo, threshold=5).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=465, shape=(2, 4), dtype=float32, numpy=\n",
       "array([[1.4945302 , 0.6327658 , 0.        , 0.12925196],\n",
       "       [0.43063554, 0.        , 0.        , 0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "layer = layers.Dense(4, activation='relu')\n",
    "\n",
    "inputs = tf.convert_to_tensor([[-1,4,5,3],[1,6,0.3,1]])\n",
    "\n",
    "outputs = layer(inputs)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21194156, 0.57611688, 0.21194156])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax : \n",
    "\n",
    "inp = np.asarray([1., 2., 1.])\n",
    "layer = tf.keras.layers.Softmax()\n",
    "layer(inp).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0611537e-09, 3.1326166e-01, 6.9314718e-01, 1.3132616e+00,\n",
       "       2.0000000e+01], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softplus : log(exp(x) + 1)\n",
    "a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)\n",
    "tf.keras.activations.softplus(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5,  0. ,  0.5], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softsign : x / (abs(x) + 1)\n",
    "\n",
    "a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)\n",
    "tf.keras.activations.softsign(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9950547, -0.7615942,  0.       ,  0.7615942,  0.9950547],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tanh : Hyperbolic tangent activation function\n",
    "# tanh : sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x))).\n",
    "\n",
    "a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n",
    "tf.keras.activations.tanh(a).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selu : Scaled Exponential Linear Unit \n",
    "# if x > 0: return scale * x\n",
    "# if x < 0: return scale * alpha * (exp(x) - 1)\n",
    "# constants (alpha=1.67326324 and scale=1.05070098)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gelu : Gaussian error linear unit (GELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hard_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04978707,  0.36787945,  1.        ,  2.7182817 , 20.085537  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exponential : exp(x).\n",
    "\n",
    "a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n",
    "\n",
    "tf.keras.activations.exponential(a).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELU :  Exponential Linear Unit \n",
    "\n",
    "# f(x) = x for x >= 0\n",
    "# f(x) =  alpha * (exp(x) - 1.) for x < 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **** Layer Weight Initilizer\n",
    "\n",
    "# Weights Initializer\n",
    "    1. kernel_initializer\n",
    "    2. bias_initializer \n",
    "\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "# Example : Weights Initializer\n",
    "model.add(layers.Dense(512, activation = 'relu', input_shape = (784,), kernel_initializer = my_init))\n",
    "\n",
    "# Normal Value\n",
    "kernel_initializer = \"normal\"\n",
    "\n",
    "# Zero Values\n",
    "kernel_initializer = initializers.Zeros() \n",
    "\n",
    "# Ones Values\n",
    "kernel_initializer = initializers.Ones() \n",
    "\n",
    "# Constant values\n",
    "kernel_initializer = initializers.Constant(value = 5) \n",
    "\n",
    "# Random Normal\n",
    "kernel_initializer = initializers.RandomNormal(mean=0.0, stddev = 0.05, seed = None) \n",
    "\n",
    "# Random Uniform\n",
    "kernel_initializer = initializers.RandomUniform(minval = -0.05, maxval = 0.05, seed = None)\n",
    "\n",
    "# Truncated Normal\n",
    "kernel_initializer = initializers.TruncatedNormal(mean = 0.0, stddev = 0.05, seed = None)\n",
    "\n",
    "#Variance Scaling\n",
    "kernel_initializer = initializers.VarianceScaling(scale = 1.0, mode = 'fan_in', distribution = 'normal', seed = None) \n",
    "\n",
    "#lecun_normal\n",
    "kernel_initializer = initializers.RandomUniform(minval = -0.05, maxval = 0.05, seed = None) \n",
    "\n",
    "# lecun_uniform\n",
    "kernel_initializer = initializers.lecun_uniform(seed = None) \n",
    "\n",
    "# glorot_normal\n",
    "kernel_initializer = initializers.glorot_normal(seed=None) \n",
    "\n",
    "# Identitty  : Initializer that generates the identity matrix.\n",
    "kernel_initializer = tf.keras.initializers.Identity(gain=1.0)\n",
    "\n",
    "# Orthogonal : nitializer that generates an orthogonal matrix.\n",
    "kernel_initializer = tf.keras.initializers.Orthogonal(gain=1.0, seed=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularizer\n",
    "    1. kernel_regularizer (Dense)\n",
    "    2. bias_regularizer  (Convolutional)\n",
    "    3. recurrent_regularizer (LSTM)\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Example : Regularizer \n",
    "model.add(layers.Dense(512,activation='relu',input_shape=(784,),kernel_regularizer=my_regularizer))\n",
    "\n",
    "# L2 Regularize\n",
    "my_regularizer = regularizers.L1(0.)\n",
    "\n",
    "# L2 Regularize\n",
    "my_regularizer = regularizers.L2(0.)\n",
    "\n",
    "#L1 and L2 Regularize\n",
    "my_regularizer = regularizers.L1L2(0.0,0.0)\n",
    "\n",
    "# Weight Regularization for Convolutional Layers\n",
    "model.add(Conv2D(32, (3,3), kernel_regularizer=regularizers.l2(0.01), bias_regularizer=regularizers.l2(0.01)))\n",
    "\n",
    "# Bias _Regularizers\n",
    "bias_regularizer=l2(0.01)\n",
    "\n",
    "# Bias _Regularizers\n",
    "bias_regularizer=l2(0.01)\n",
    "\n",
    "# Weight Regularization for Recurrent Layers\n",
    "model.add(LSTM(32, kernel_regularizer=l2(0.01), recurrent_regularizer=regularizers.l2(0.01),\n",
    "                   bias_regularizer=regularizersl2(0.01)))\n",
    "\n",
    "# Recurrent Regularization for LSTM\n",
    "recurrent_regularizer=l2(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer Weight constraints\n",
    "# Weight constraints provide an approach to reduce the overfitting of a deep learning.\n",
    "\n",
    "model.add(Dense(32, kernel_constraint=max_norm(3), bias_constraint=max_norm(3))) # Dense\n",
    "model.add(Conv2D(32, (3,3), kernel_constraint=max_norm(3), bias_constraint=max_norm(3))) # CNN\n",
    "model.add(LSTM(32, kernel_constraint=max_norm(3), recurrent_constraint=max_norm(3), bias_constraint=max_norm(3))) # LSTM\n",
    "\n",
    "# MaxNorm : Weights incident to each hidden unit to have a norm less than or equal to a desired value\n",
    "tf.keras.constraints.MaxNorm(max_value=2, axis=0)\n",
    "\n",
    "# MinMaxNorm: Constrains the weights incident to each hidden unit to have the norm between a lower bound and an upper bound.\n",
    "tf.keras.constraints.MinMaxNorm(min_value=0.0, max_value=1.0, rate=1.0, axis=0)\n",
    "\n",
    "# NonNeg : Constrains the weights to be non-negative\n",
    "\n",
    "# RadialConstraint: Constrains Conv2D kernel weights to be the same for each radius.\n",
    "\n",
    "# unitNorm : Constrains the weights incident to each hidden unit to have unit norm.\n",
    "tf.keras.constraints.UnitNorm(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model : Model groups layers into an object with training and inference features.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define Input:\n",
    "inputs = tf.keras.Input(shape=(3,))\n",
    "\n",
    "# Define output\n",
    "outputs = tf.keras.layers.Dense(2)(inputs)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(3,))\n",
    "    \n",
    "d = tf.keras.layers.Dense(2, name='out')\n",
    "\n",
    "output_1 = d(inputs)\n",
    "output_2 = d(inputs)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=[output_1, output_2])\n",
    "model.compile(optimizer=\"Adam\", loss=\"mse\", metrics=[\"mae\", \"acc\"])\n",
    "\n",
    "import numpy as np\n",
    "x = np.random.random((2, 3))\n",
    "y = np.random.randint(0, 2, (2, 2))\n",
    "\n",
    "model.fit(x, (y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=13, kernel_initializer=\"normal\",activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1, kernel_initializer=\"normal\"))\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Model Functions\n",
    "'''\n",
    "model.add() \n",
    "model.get_weights()\n",
    "model.compile()\n",
    "model.fit()\n",
    "model.evaluate()\n",
    "model.predict()\n",
    "model.summary()\n",
    "model.save()\n",
    "model.save_weights()\n",
    "model.build()\n",
    "model.get_weights()\n",
    "model.add_loss()\n",
    "model.add_metric()\n",
    "model.to_json()\n",
    "model.to_yaml()\n",
    "model.metrics()\n",
    "model.optimizer()\n",
    "model.count_params()\n",
    "\n",
    "model.input_shape()\n",
    "model.load_weights()\n",
    "model.get_output_at(1)\n",
    "model.get_laye[0].get_weights()\n",
    "model.layers[0].get_input_at(0)\n",
    "model.layers[0].get_output_at(0)\n",
    "model.layers[2].units\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Input\n",
    "\n",
    "input_dim=13\n",
    "input_shape = (13,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.24116312  1.3015687   0.7565762  -0.47319087]\n",
      "  [ 0.97014725  0.28704515 -0.25953227 -0.6662777 ]\n",
      "  [-1.368204    2.3441765  -2.0411115   0.38773426]]\n",
      "\n",
      " [[-0.1476079  -0.5302426   0.43511036  0.13674329]\n",
      "  [ 1.172246    0.40507308  1.06943     0.4349389 ]\n",
      "  [ 0.08766053 -1.7162492  -0.6831919  -0.7756794 ]]], shape=(2, 3, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.keras.layers.Add\n",
    "\n",
    "# Layer that adds a list of inputs.\n",
    "# It takes as input a list of tensors, all of the same shape, and returns a single tensor \n",
    "\n",
    "input_shape = (2,3, 4)\n",
    "\n",
    "x1 = tf.random.normal(input_shape)\n",
    "x2 = tf.random.normal(input_shape)\n",
    "\n",
    "y = tf.keras.layers.Add()([x1, x2])\n",
    "\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Functional API implementing Add Function\n",
    "\n",
    "input1 = tf.keras.layers.Input(shape=(16,))\n",
    "x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n",
    "\n",
    "input2 = tf.keras.layers.Input(shape=(32,))\n",
    "x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n",
    "\n",
    "added = tf.keras.layers.Add()([x1, x2])\n",
    "\n",
    "out = tf.keras.layers.Dense(4)(added)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.Average : Layer that averages a list of inputs element-wise.\n",
    "\n",
    "input1 = tf.keras.layers.Input(shape=(16,))\n",
    "x1 = tf.keras.layers.Dense(8, activation='relu')(input1)\n",
    "\n",
    "input2 = tf.keras.layers.Input(shape=(32,))\n",
    "x2 = tf.keras.layers.Dense(8, activation='relu')(input2)\n",
    "\n",
    "avg = tf.keras.layers.Average()([x1, x2])\n",
    "\n",
    "out = tf.keras.layers.Dense(4)(avg)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bathc Normalization :\n",
    "\n",
    "# Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for \n",
    "# each mini-batch.This has the effect of stabilizing the learning process and dramatically reducing the \n",
    "# number of training epochs required to train deep networks.\n",
    "\n",
    "# Batch normalization applies a transformation that maintains the mean output close to 0 and the output standard deviation close to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate\n",
    "\n",
    "# Layer that concatenates a list of inputs.\n",
    "\n",
    "x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n",
    "x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n",
    "\n",
    "concatted = tf.keras.layers.Concatenate()([x1, x2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=854, shape=(1, 2, 2), dtype=int32, numpy=\n",
       "array([[[260, 360],\n",
       "        [320, 445]]])>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dot\n",
    "\n",
    "x = np.arange(10).reshape(1, 5, 2)\n",
    "y = np.arange(10, 20).reshape(1, 2, 5)\n",
    "\n",
    "tf.keras.layers.Dot(axes=(1, 2))([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [2. 3.]\n",
      " [4. 5.]\n",
      " [6. 7.]\n",
      " [8. 9.]]\n",
      "\n",
      "\n",
      "After Dropout:\n",
      "tf.Tensor(\n",
      "[[ 0.    1.25]\n",
      " [ 2.5   3.75]\n",
      " [ 5.    0.  ]\n",
      " [ 0.    8.75]\n",
      " [10.    0.  ]], shape=(5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Dropout\n",
    "\n",
    "# The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time,\n",
    "# which helps prevent overfitting\n",
    "\n",
    "data = np.arange(10).reshape(5, 2).astype(np.float32)\n",
    "\n",
    "layer = tf.keras.layers.Dropout(.2, input_shape=(2,))\n",
    "\n",
    "print(data)\n",
    "\n",
    "outputs = layer(data, training=True)\n",
    "\n",
    "print(\"\\n\")\n",
    "print('After Dropout:')\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum\n",
    "\n",
    "# Functional interface to compute maximum (element-wise) list of inputs.\n",
    "\n",
    "input1 = tf.keras.layers.Input(shape=(16,))\n",
    "x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)\n",
    "\n",
    "input2 = tf.keras.layers.Input(shape=(32,))\n",
    "x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)\n",
    "\n",
    "max_inp=tf.keras.layers.maximum([x1,x2]) #shape=(None, 8)\n",
    "\n",
    "out = tf.keras.layers.Dense(4)(max_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1037, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[ 7,  8,  9],\n",
       "       [10, 11, 12]])>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximum\n",
    "\n",
    "inout1 = tf.convert_to_tensor([[1,2,9],[4,11,6]])\n",
    "inout2 = tf.convert_to_tensor([[7,8,9],[10,11,12]])\n",
    "\n",
    "tf.keras.layers.maximum([inout1,inout2]) #shape=(None, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1042, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[ 1,  2,  9],\n",
       "       [ 4, 11,  6]])>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimum : Layer that computes the minimum (element-wise) a list of inputs.\n",
    "\n",
    "inout1 = tf.convert_to_tensor([[1,2,9],[4,11,6]])\n",
    "inout2 = tf.convert_to_tensor([[7,8,9],[10,11,12]])\n",
    "\n",
    "tf.keras.layers.minimum([inout1,inout2]) #shape=(None, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum\n",
    "\n",
    "x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n",
    "x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n",
    "\n",
    "minned = tf.keras.layers.Minimum()([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1049, shape=(2, 3), dtype=int32, numpy=\n",
       "array([[  7,  16,  81],\n",
       "       [ 40, 121,  72]])>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply : Layer that multiplies (element-wise) a list of inputs.\n",
    "\n",
    "inout1 = tf.convert_to_tensor([[1,2,9],[4,11,6]])\n",
    "inout2 = tf.convert_to_tensor([[7,8,9],[10,11,12]])\n",
    "\n",
    "tf.keras.layers.multiply([inout1,inout2]) #shape=(None, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply :\n",
    "\n",
    "x1 = tf.keras.layers.Dense(8)(np.arange(10).reshape(5, 2))\n",
    "x2 = tf.keras.layers.Dense(8)(np.arange(10, 20).reshape(5, 2))\n",
    "\n",
    "multiplied = tf.keras.layers.Multiply()([x1, x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract : Layer that Subtract (element-wise) a list of inputs.\n",
    "\n",
    "inout1 = tf.convert_to_tensor([[1,2,9],[4,11,6]])\n",
    "inout2 = tf.convert_to_tensor([[7,8,9],[10,11,12]])\n",
    "\n",
    "tf.keras.layers.Subtract([inout1,inout2]) #shape=(None, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before RepeatVector\n",
      "(None, 32)\n",
      "After RepeatVector\n",
      "(None, 3, 32)\n"
     ]
    }
   ],
   "source": [
    "# RepeatVector: Repeats the input n times.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(32, input_dim=8))\n",
    "\n",
    "# add RepeatVector\n",
    "print('Before RepeatVector')\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(tf.keras.layers.RepeatVector(3))\n",
    "\n",
    "print('After RepeatVector')\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 3, 4)\n",
      "(None, 6, 2)\n",
      "(None, None, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Reshape : Layer that reshapes inputs into the given shape.\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))\n",
    "\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((6, 2)))\n",
    "\n",
    "print(model.output_shape)\n",
    "\n",
    "model.add(tf.keras.layers.Reshape((-1, 2, 2)))\n",
    "\n",
    "print(model.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.5, 0.5], [0.5, 0.5]]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average\n",
    "\n",
    "x1 = np.ones((2, 2))\n",
    "x2 = np.zeros((2, 2))\n",
    "\n",
    "y = tf.keras.layers.Average()([x1, x2])\n",
    "\n",
    "y.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattern : Flattens the input. Does not affect the batch size.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers \n",
    "\n",
    "# Adadelta : Extension of Adagrad : Adadelta(adaptive delta)\n",
    "# Adagrad  : Adaptive Gradient Algorithm\n",
    "# Adam     : Adaptive Moment Estimation\n",
    "# Adamax   :\n",
    "# Ftrl     : Follow The Regularized Leader\n",
    "# Nadam    : Nesterov-accelerated Adaptive Moment Estimation\n",
    "# RMSprop  : Root Mean Square Propagation \n",
    "# SGD      : Stochastic Gradient Descent\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "adadelta = Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta',**kwargs)\n",
    "# Adadelta\n",
    "model.compile(optimizer='adadelta',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "adagrad = Adagrad(learning_rate=0.001,initial_accumulator_value=0.1,epsilon=1e-07,name=\"Adagrad\",**kwargs) \n",
    "# Adagrad\n",
    "model.compile(optimizer='adagrad',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#Adam\n",
    "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam', **kwargs) \n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "# AdaAmx\n",
    "adamax = Adamax(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,name='Adamax', **kwargs) \n",
    "model.compile(optimizer=adamax,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.optimizers import Ftrl\n",
    "# Ftrl\n",
    "ftrl = Ftrl(learning_rate=0.001,learning_rate_power=-0.5,initial_accumulator_value=0.1,l1_regularization_strength=0.0,\n",
    "            l2_regularization_strength=0.0,name=\"Ftrl\",l2_shrinkage_regularization_strength=0.0,beta=0.0,**kwargs)\n",
    "model.compile(optimizer=ftrl,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "# Nadam\n",
    "nadam = Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07,name='Nadam', **kwargs) \n",
    "model.compile(optimizer=nadam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "# RMSProp\n",
    "rmsprop = RMSprop(learning_rate=0.001,rho=0.9, momentum=0.0, epsilon=1e-07, \n",
    "                  centered=False,name='RMSprop', **kwargs) \n",
    "model.compile(optimizer=rmsprop,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "# SGD\n",
    "sgd = SGD(lr =0.01,decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "#1. mean_squared_error\n",
    "#2. mean_absolute_error\n",
    "#3. mean_absolute_percentage_error\n",
    "#4. mean_squared_logarithmic_error\n",
    "#5. squared_hinge\n",
    "#6. hinge\n",
    "#7. categorical_hinge\n",
    "#8. logcosh\n",
    "#9. huber_loss\n",
    "#10. categorical_crossentropy\n",
    "#11. sparse_categorical_crossentropy\n",
    "#12. binary_crossentropy\n",
    "#13. kullback_leibler_divergence\n",
    "#14. poisson\n",
    "#15. cosine_proximity\n",
    "#16. is_categorical_crossentropy\n",
    "\n",
    "\n",
    "- Regression\n",
    "    - mean_squared_error\n",
    "    - mean_absolute_error\n",
    "    - mean_absolute_percentage_error\n",
    "    - mean_squared_logarithmic_error\n",
    "    - cosine_proximity\n",
    "    - huber_loss\n",
    "    - logcosh\n",
    "- Binary and Multi Classification\n",
    "    - binary_crossentropy\n",
    "    - categorical_crossentropy\n",
    "    - poisson loss\n",
    "    - sparse_categorical_crossentropy\n",
    "    - kullback_leibler_divergence\n",
    "- Hinge losses \n",
    "    - Hinge\n",
    "    - Categorical Hinge\n",
    "    - Squared Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.metrics.BinaryCrossentropy"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Metrics\n",
    "\n",
    "from keras import metrics\n",
    "\n",
    "metrics.BinaryCrossentropy\n",
    "\n",
    "# AUC : Area Under Curve\n",
    "# Accuracy\n",
    "# Binary Accuracy\n",
    "# BinaryCrossEntropy\n",
    "# CategoricalAccuracy\n",
    "# CategoricalCrossEntropy\n",
    "# CategoricalHinge\n",
    "# CosineSimilarity\n",
    "# FalseNegative\n",
    "# FalsePositive\n",
    "# Hinge\n",
    "# KLDivergence\n",
    "# LogCoshError\n",
    "# Mean\n",
    "# MeanAbolsuteError\n",
    "# MeanAbsolutePercentageError\n",
    "# MeanIou\n",
    "# MeanRelatievError\n",
    "# MeanSqaredError\n",
    "# MeanSquaredLogarithmicError\n",
    "# MeanTensor\n",
    "# Poission\n",
    "# PrecisionAtRecall\n",
    "# Recall\n",
    "# RecallAtPrecision\n",
    "# RootMeanSquaredError\n",
    "# SensitivityAtSpecificity\n",
    "# SparseCatgoricalAccuracy\n",
    "# SparseTopKCategoricalAccuracy\n",
    "# SpecificityAtSensivity\n",
    "# SquardHinge\n",
    "# Sum\n",
    "# TopKCategoricalAccuracy\n",
    "# TrueNegative\n",
    "# TruePositives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
