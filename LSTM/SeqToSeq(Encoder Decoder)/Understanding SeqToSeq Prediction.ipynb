{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 01 Encoder and Decoder LSTMs : Seq2Seq\n",
    "\n",
    "One LSTM network encodes input sequences and a separate LSTM network decodes the encoding into an output sequence\n",
    "\n",
    "The encoder-decoder model is a way of organizing recurrent neural networks for sequence-to-sequence prediction problems.\n",
    "\n",
    "# Architecture of Encoder decoder LSTM\n",
    "\n",
    "Input--> Encoder Model-->Decoder Model-->Dense-->output\n",
    "\n",
    "# Application\n",
    "    - Machine Translation\n",
    "    - Learning To Execute\n",
    "    - Image Caption\n",
    "    - Conversational Modeling\n",
    "    - Movement Classification\n",
    "    \n",
    "The approach involves two recurrent neural networks, \n",
    "    1. one to encode the source sequence, called the encoder, \n",
    "    2. second to decode the encoded source sequence into the target sequence, called the decoder.\n",
    "    \n",
    "    # Model Structure\n",
    "    1. Define training encoder\n",
    "    2. Define training decoder\n",
    "    3. Define inference decoder\n",
    "    4. Define inference loop to decode_sequence\n",
    "    \n",
    "    \n",
    "Encoder-Decoder model consist of two LSTMs. One will serve as an encoder,encoding the input sequence and producing internal \n",
    "state vectors which serve as conditioning for the decoder.\n",
    "\n",
    "The decoder, another LSTM, is responsible for predicting the individual characters of the target sequence. Its initial state \n",
    "is set to the state vectors from the encoder\n",
    "\n",
    "Note : \n",
    "    1.Create encoder and get the encoder's internal state vectors\n",
    "    2.use the encoder's internal state vectors for the decoder.\n",
    "\n",
    "# Define Training Encoder :\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "Note : \n",
    "    1. Use the encoder's internal state vectors for the decoder.\n",
    "    2. Build the Decoder using encoder_states \n",
    "    3. during the cretaing LSTM and return output at decoder we're not using the decoder's internal states during training\n",
    "       but we will need them later for inference.\n",
    "    \n",
    "    4. Decoder's output we attach a Dense layer to the decoder's LSTM outputs.\n",
    "\n",
    "# Define Trainig Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states) #nternal states not using for trainig\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Keras Function API\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "\n",
    "Intermediate vector : This is the final internal state produced from the encoder part of the model. \n",
    "                      It contains information about the entire input sequence to help the decoder make accurate predictions\n",
    "        \n",
    "\n",
    "# Define inference decoder\n",
    "decoder_state_input_h = Input(shape=(n_units,))\n",
    "decoder_state_input_c = Input(shape=(n_units,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Generate target given source sequenc using encode internal states and inference decoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
