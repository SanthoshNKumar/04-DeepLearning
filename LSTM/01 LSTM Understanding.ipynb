{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence : Order\n",
    "\n",
    "# Sequence Prediction Problems :\n",
    "\n",
    "1. Sequence Prediction : Predicting next value for a given input sequence\n",
    "        Ex: Input Sequence : 12345 Output Sequence : 6\n",
    "            \n",
    "2. Sequence Classification : Predicting class label for a given input sequence\n",
    "        Ex: Input Sequence : 12345 Output Sequence : \"Good\" or \"Bad\"\n",
    "\n",
    "3. Sequence Generation :generating a new output sequence that has the same general characteristics \n",
    "                        as other sequences in the corpus\n",
    "        Ex: Input Sequence: [1, 3, 5], [7, 9, 11] Output Sequence: [3, 5 ,7]\n",
    "                    \n",
    "4. Sequence-to-Sequence Prediction : Predicting an output sequence given an input sequence\n",
    "        Ex: Input Sequence: 1, 2, 3, 4, 5 Output Sequence: 6, 7, 8, 9, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.Sequence Prediction Probelms :\n",
    "    1. Weather Forecasting\n",
    "    2. Stock market Prediction\n",
    "    3. Product Recommendation\n",
    "    \n",
    "2.Sequence Classification :\n",
    "    1. DNA Sequence Classification :Given a DNA sequence of A, C, G, and T values, predict whether the sequence \n",
    "                                    is for a coding or non-coding region\n",
    "            \n",
    "    2. Anomaly Detection : Given a sequence of observations, predict whether the sequence is anomalous or not\n",
    "        \n",
    "    3. Sentiment Analysis : Given a sequence of text such as a review or a tweet, predict whether the sentiment \n",
    "                            of the text is positive or negative          \n",
    "3.Sequence Generation : \n",
    "    1. Text Generation \n",
    "    2. Handwriting Prediction.\n",
    "    3. Music Generation\n",
    "    4. Image Caption Generation : Input Sequence: [image pixels] Output Sequence: [\"man riding a bike\"]\n",
    "                \n",
    "\n",
    "4.Sequence-to-Sequence Prediction :\n",
    "    1. Multi-Step Time Series Forecasting\n",
    "    2. Text Summarization\n",
    "    3. Program Execution.\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LSTM Weight : A memory cell has weight parameters for the input,output and Intermediate step\n",
    "\n",
    "    1.Input Weights : Used to weight input for the current time step\n",
    "    \n",
    "    2.Output Weights : Used to weight the output from the last time step.\n",
    "    \n",
    "    3.Internal State. Internal state used in the calculation of the output for this time step"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "LSTM Gates : The key to the memory cell are the gates These too are weighted functions that further gover the information flow\n",
    "             in the cell.\n",
    "\n",
    "    1.Forget Gate: Decides what information to discard from the cell.\n",
    "    \n",
    "    2.Input Gate:  Decides which values from the input to update the memory state.\n",
    "    \n",
    "    3.Output Gate: Decides what to output based on input and the memory of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Backpropagation Training Algorithm :\n",
    "    \n",
    "    1. The mathematical method used to calculate derivatives and an application of the derivative chain rule.\n",
    "    2. The training algorithm for updating network weights to minimize error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps Followed in the General Algorithms\n",
    "\n",
    "1. Present a training input pattern and propagate it through the network to get an output\n",
    "2. Compare the predicted outputs to the expected outputs and calculate the error\n",
    "3. Calculate the derivatives of the error with respect to the network weights.\n",
    "4. Adjust the weights to minimize the error\n",
    "5. Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for LSTM\n",
    "\n",
    "1. Numeric Data : Data needs to be scaled when training a neural network\n",
    "\n",
    "2. Categorical Data : Convert Categorical Data to Numerical Data\n",
    "       - Integer Encoding\n",
    "       - One Hot Encoding\n",
    "\n",
    "3. Sequences with Varied Lengths\n",
    "       - Sequence Padding\n",
    "            - Pre-Sequence Padding\n",
    "            - Post-Sequence Padding\n",
    "       - Sequence Truncation\n",
    "            - Pre-Sequence Truncation\n",
    "            - Post-Sequence Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   t  t-1  t+1\n",
      "0  0  NaN  1.0\n",
      "1  1  0.0  2.0\n",
      "2  2  1.0  3.0\n",
      "3  3  2.0  4.0\n",
      "4  4  3.0  5.0\n",
      "5  5  4.0  6.0\n",
      "6  6  5.0  7.0\n",
      "7  7  6.0  8.0\n",
      "8  8  7.0  9.0\n",
      "9  9  8.0  NaN\n"
     ]
    }
   ],
   "source": [
    "# For Seqeunce data preparing input and output column use pandas shift()\n",
    "\n",
    "from pandas import DataFrame\n",
    "# define the sequence\n",
    "df = DataFrame()\n",
    "df['t'] = [x for x in range(10)]\n",
    "# print(df)\n",
    "\n",
    "# Shift Fowrds\n",
    "df['t-1'] = df['t'].shift(1)\n",
    "\n",
    "# Shift backward\n",
    "df['t+1'] = df['t'].shift(-1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Steps Involved in Developing LSTM Model\n",
    "\n",
    "1. Define the Model \n",
    "2. Compile Model\n",
    "3. Fit Model\n",
    "4. Evaluate Model\n",
    "5. Make Prediction with the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the Model \n",
    "\n",
    "Neural net works in keras as sequence of layers,The container for these layers is the Sequential class.\n",
    "\n",
    "\n",
    "1. Create an instance of the Sequential class (container for layers)\n",
    "2. Create multiple layers and add them in the order that they should be connected.(Ex LSTM Layer)\n",
    "3. Dense is fully connected layer that often follows LSTMlayers used for outputting a prediction\n",
    "\n",
    "\n",
    "*************************************************************************************************************\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "\n",
    "# 01 : One Way of Creating\n",
    "# we can define an LSTM hidden layer with 2 memory cells followed by a Dense output layer with 1 neuron\n",
    "model = Sequential()\n",
    "model.add(LSTM(2))\n",
    "model.add(Dense(1))\n",
    "\n",
    "#02 : creating an array of layers and passing it to the constructor of the Sequential class\n",
    "layers = [LSTM(2), Dense(1)]\n",
    "model = Sequential(layers)\n",
    "*************************************************************************************************************\n",
    "\n",
    "\n",
    "# Input : Input must be three-dimensional (Samples,Time Step, Features)\n",
    "\n",
    "# Activation Functions :\n",
    "    Regression: Linear activation function\n",
    "    Binary Classification : sigmoid\n",
    "    Multiclass Classification : Softmax"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Compile the Model\n",
    "\n",
    "Once we have defined our network, we must compile it.\n",
    "\n",
    "It Tranforms the simple Sequences of layers that we defined into a highly efficient series of matrix transforms in a format intended to be executed on your GPU or CPU.\n",
    "\n",
    "Required Parameters for compilation\n",
    "    - Optimization Algorithms\n",
    "    - Loss Function(Evaluate the network that is minimized by the Optimization Algorithm)\n",
    "    - Metrics : Specify metrics to collect while fitting your model in addition to the loss function.\n",
    " \n",
    "Optimization Algorithms:\n",
    "   - Stochastic Gradient Descent, or sgd\n",
    "   - Adam, or adam.\n",
    "   - RMSprop, or rmsprop.\n",
    "\n",
    "Loss Function:\n",
    "   - Regression: mean squared error\n",
    "   - Binary Classification : binary crossentropy\n",
    "   - Multiclass Classification : categorical crossentropy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Fit the Model\n",
    "\n",
    "Once the network is compiled, it can be fit, which means that adapting the weighths on training dataset.\n",
    "\n",
    "The network is trainined using the backpropagation through sequence algorithm and optimized according to the optimization algorithm and loss function specified when compiling the model.\n",
    "\n",
    "Required parameters for Fitting:\n",
    "    - Training Data (Input Pattern(X) and output Pattern(Y))\n",
    "    - Epoch: One pass through all samples in the training dataset and updating the network weights\n",
    "    - Batch : A pass through a subset of samples in the training dataset after which the network weights are updated.\n",
    "   \n",
    "batch_size = 1 (Weights are updated after each sample and the procedure is called stochastic gradient descent)\n",
    "batch_size = 32 (Weights areupdatedafter a specifiednumber of samples andtheprocedure is called mini-batch gradient descent)\n",
    "batch size = n (Where n is the number of samples in the training dataset it is called  batch gradient descent)\n",
    "\n",
    "Once fit Done, a history object is returned that provides a summary of the performance of the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "\n",
    "Once the network is trained, it can be evaluated\n",
    "\n",
    "# Predict the Model\n",
    "Predict model is calling the predict() function on the model with an array of new input patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM State Management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Data for LSTM\n",
    "\n",
    "1. The LSTM input layer must be 3D.\n",
    "2. The meaning of the 3 input dimensions are: samples, time steps and features.\n",
    "3. The reshape() function on NumPy arrays can be used to reshape your 1D or 2D data to be 3D.\n",
    "4. The reshape() function takes a tuple as an argument that defines the new shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-One (Multiple Features)\n",
    "\n",
    "X = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "\n",
    "X = array(X).reshape(20, 1, 1)\n",
    "\n",
    "model.add(LSTM(50, activation='relu', input_shape=(1, 1)))  # input_shape=(1 time_step,2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-to-One (Multiple Features)\n",
    "\n",
    "X1 = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]\n",
    "X2 = [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75]\n",
    "\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "X = array(X).reshape(25, 1, 2) # convert the input into 3-dimensional shape\n",
    "\n",
    "model.add(LSTM(80, activation='relu', input_shape=(1, 2))) # input_shape=(1 time_step,2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many-to-One Sequence (Single Feature)\n",
    "\n",
    "X = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,\n",
    "     36,37,38,39,40,41,42,43,44,45]\n",
    "\n",
    "y = [6,15,24,33,42,51,60,69,78,87,96,105,114,123,132]\n",
    "\n",
    "X = X.reshape(15,3,1)\n",
    "\n",
    "model.add(LSTM(50, activation='relu', input_shape=(3, 1))) # input_shape=(3 time_step,1 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many-to-One Sequence (Multiple Feature)\n",
    "\n",
    "X1  = [3,6,9,12,15,18,21,24,27,30,33,36,39,42,45,48,51,54,57,60,63,66,69,72,75,78,81,84,87,90,93,\n",
    "       96,99,102,105,108,111,114,117,120,123,126,129,132,135]\n",
    "\n",
    "X2  = [5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110,115,120,125,130,135,\n",
    "       140,145,150,155,160,165,170,175,180,185,190,195,200,205,210,215,220,225]\n",
    "\n",
    "y = [24,48,72,96,120,144,168,192,216,240,264,288,312,336,360]\n",
    "\n",
    "X = np.column_stack((X1, X2))\n",
    "\n",
    "X = array(X).reshape(15, 3, 2)\n",
    "\n",
    "model.add(LSTM(50, activation='relu', input_shape=(3, 2))) # input_shape=(3 time_step,2 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import array\n",
    "\n",
    "data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "\n",
    "data = data.reshape((1, 10, 1)) # 1 sample,10 time steps, and 1 features\n",
    "\n",
    "model.add(LSTM(32, input_shape=(10, 1))) # input_shape=(10 time_step, 1 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "X = array([[0.1, 1.0],[0.2, 0.9],[0.3, 0.8],[0.4, 0.7],[0.5, 0.6],[0.6, 0.5],[0.7, 0.4],[0.8, 0.3],[0.9, 0.2],[1.0, 0.1]])\n",
    "\n",
    "# 1 sample with 10 time steps and 2 features.\n",
    "X = X.reshape(1, 10, 2)\n",
    "\n",
    "model.add(LSTM(32, input_shape=(10, 2)) # input_shape=(10 time_step, 2 features)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note:\n",
    "    \n",
    "1.The input of the LSTM is always is a 3D array. (batch_size, time_steps, units)\n",
    "\n",
    "2.The output of the LSTM could be a 2D array or 3D array depending upon the return_sequences argument.\n",
    "\n",
    "3.If return_sequence is False, the output is a 2D array. (batch_size, units)\n",
    "\n",
    "4.If return_sequence is True, the output is a 3D array. (batch_size, time_steps, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example :\n",
    "\n",
    "# Generated sequences as follows\n",
    "\n",
    "# One Sample Input Sequence in raw format:\n",
    "X[0]=[5, 0, 9, 9]\n",
    "\n",
    "# One Hot Encoding\n",
    "\n",
    "5 : [0 0 0 0 0 1 0 0 0 0]\n",
    "0 : [1 0 0 0 0 0 0 0 0 0]\n",
    "9 : [0 0 0 0 0 0 0 0 0 1]\n",
    "9 : [0 0 0 0 0 0 0 0 0 1]\n",
    "\n",
    "# In one_hot_encoded format:  # (1,4,10) (1 Sample ; 4 time steps ; 10 feaures)\n",
    "\n",
    "X[0]=[[0 0 0 0 0 1 0 0 0 0],\n",
    "      [1 0 0 0 0 0 0 0 0 0],\n",
    "      [0 0 0 0 0 0 0 0 0 1],\n",
    "      [0 0 0 0 0 0 0 0 0 1]]\n",
    "\n",
    "# Shape of an input to LSTM (X[0].shape):  (1, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 10, 32)\n"
     ]
    }
   ],
   "source": [
    "# Example 2:\n",
    "\n",
    "# embedding of the Characters in the string : for LSTM\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "word_vec_length = 10 # maximum length of the name\n",
    "\n",
    "char_to_int = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, \n",
    "               'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, \n",
    "               'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, \n",
    "               'w': 22, 'x': 23, 'y': 24, 'z': 25, 'ö': 26, 'ä': 27, 'ü': 28,\n",
    "               '-': 29, 'ß': 30, ' ': 31}\n",
    "\n",
    "char_vec_length = len(char_to_int)\n",
    "\n",
    "\n",
    "# Returns a list of n lists with n = word_vec_length\n",
    "def name_encoding(name):\n",
    "\n",
    "    # Encode input data to int, e.g. a->1, z->26\n",
    "    integer_encoded = [char_to_int[char] for i, char in enumerate(name) if i < word_vec_length]\n",
    "    \n",
    "    # Start one-hot-encoding\n",
    "    onehot_encoded = list()\n",
    "    \n",
    "    for value in integer_encoded:\n",
    "        # create a list of n zeros, where n is equal to the number of accepted characters\n",
    "        letter = [0 for _ in range(char_vec_length)]\n",
    "        letter[value] = 1\n",
    "        onehot_encoded.append(letter)\n",
    "        \n",
    "    # Fill up list to the max length. Lists need do have equal length to be able to convert it into an array\n",
    "    for _ in range(word_vec_length - len(name)):\n",
    "        onehot_encoded.append([0 for _ in range(char_vec_length)])\n",
    "        \n",
    "    return onehot_encoded\n",
    "\n",
    "names = ['santhosh','anand','sachin','prashanth']\n",
    "\n",
    "# use asarray to use change the shape of the array which suits for LSTM\n",
    "enc = np.asarray([np.asarray(name_encoding(name)) for name in names])\n",
    "\n",
    "print(enc.shape)\n",
    "\n",
    "# Batch_size(Samples) : 4  : ['santhosh','anand','sachin','prashanth']\n",
    "# time_step : 10 :  max length of the sentence \n",
    "# features : 32 :  Length of the Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(enc[0]) # First Sample Data # Row(time_step) : 10 and Column(Features) :32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(enc[3]) # Forth Sample Data # Row(time_step) : 10 and Column(Features) :32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 26)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One Hot Encoding Creation machanisam for LSTM\n",
    "\n",
    "names = [\"santhosh\",\"kumar\",\"anand\",\"joshi\"] # Samples\n",
    "\n",
    "import string \n",
    "\n",
    "# get the alphabest\n",
    "string.ascii_lowercase\n",
    "\n",
    "# List of Vocabulary\n",
    "vocabs_ = [x for x in string.ascii_lowercase] # Features\n",
    "\n",
    "# vocabulary length\n",
    "vocab_length = len(string.ascii_lowercase)\n",
    "\n",
    "# Max Sequence Length\n",
    "max_seq_length = max([len(x) for x in names])\n",
    "\n",
    "char_2_int = {} \n",
    "int_2_char = {}\n",
    "\n",
    "# Char to Interger \n",
    "char_2_int = dict([(x,i) for i,x in enumerate(vocabs_)])\n",
    "\n",
    "# Integer to Char\n",
    "int_2_char = dict([(i,x) for i,x in enumerate(vocabs_)])\n",
    "\n",
    "# Create Zero array for encoding\n",
    "\n",
    "encoding = np.zeros((len(names),max_seq_length,vocab_length))\n",
    "\n",
    "print(encoding.shape)  # (4 Samples,max length of samples,vocabulary Length\n",
    "\n",
    "# Update the encoder matrix for the samples\n",
    "\n",
    "for i,name in enumerate(names): # i = index value and i = sample and j =time step\n",
    "    for j,char in enumerate(name):\n",
    "        encoding[i,j,char_2_int[char]] = 1.0\n",
    "        \n",
    "encoding"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Models for Sequence Prediction\n",
    "\n",
    "Sequence Porblem brodly Categories into the following categories\n",
    "\n",
    "1. One-to-One : Where there is a one input and one output. \n",
    "                Example : where you have a image and you want to predict a single label for the image\n",
    "\n",
    "2. Many-to-One : We have sequence of data as input and we have to predict a single output.\n",
    "                 Example : Text classification is a primary example of many to one sequence example \n",
    "\n",
    "3. One-to-Many : We have Single input and many a sequence of outputs.\n",
    "                 Example : image and its corresponding description\n",
    "\n",
    "4. Many-to-Many : It involves sequecne of input and a sequence of output.\n",
    "                 Example : Stock price of 7 days as input and stock price of next 7 days as output\n",
    "\n",
    "Chatbot is also an exmple for Many to Many sequence problem where a text sequecne is an input and another text sequence is the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping Application to Model\n",
    "\n",
    "1. Univariate Time Series Forecasting\n",
    "    Input : Multiple input time steps\n",
    "    Output : one time step\n",
    "    Model : Many-To-One model (Single Feature)\n",
    "        \n",
    "2. Multivariate Time Series Forecasting\n",
    "    Input : Multiple series with multiple input time steps\n",
    "    Output : one time step\n",
    "    Model : many-to-one model (Multiple Features)\n",
    "        \n",
    "3. Multi-step Time Series Forecasting\n",
    "   Input : One or Multiple series with multiple input time steps\n",
    "   Output : Multiple time steps\n",
    "   Model : many to Many Model\n",
    "\n",
    "4. Time Series Classification\n",
    "   Input : Multiple series with multiple input time steps\n",
    "   Output : Classification Label\n",
    "   Model : many to many Model\n",
    "    \n",
    "5. Image Captioning\n",
    "   Input : one image\n",
    "   Output : Textual description.\n",
    "   Model : One to many Model\n",
    "\n",
    "6. Sentiment Analysis\n",
    "   Input : sequences of text\n",
    "   Output : classification label \n",
    "   Model : Many to One Model\n",
    "\n",
    "7. Speech Recognition\n",
    "   Input : Sequence of audio data\n",
    "   Output : Generate a textual description\n",
    "   Model : Many to Many Model\n",
    "\n",
    "8. Text Translation\n",
    "   Input : Sequence of words in one language\n",
    "   Output : sequence of words in another language\n",
    "   Model : Many to Many model\n",
    "\n",
    "8. Text Summarization\n",
    "   Input : Document of text\n",
    "   Output : Short textual summary of the document\n",
    "   Model : Many To Many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different LSTMs Architectures\n",
    "\n",
    "# 01 Vanilla LSTMs : Memory cells of a single LSTM layer are used in a simple network structure.\n",
    "    \n",
    "Vanilla LSTM consists of:\n",
    "    1. Input Layer\n",
    "    2. Fully Connected LSTM Hidden Layer\n",
    "    3. Fully Connected output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02 Stcked LSTM\n",
    "\n",
    "The Stacked LSTM is a model that has multiple hidden LSTM layers where each layer contains multiple memory cells."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 03 CNN LSTMs :\n",
    "\n",
    "The CNN LSTM architecture involves using Convolutional Neural Network (CNN) layers for feature extraction on input data combined with LSTMs to support sequence prediction.\n",
    "\n",
    "CNN LSTMs were developed for visual time series prediction problems and the application of generating textual descriptions from sequences of images (e.g. videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 04 Encoder and Decoder LSTMs : \n",
    "\n",
    "One LSTM network encodes input sequences and a separate LSTM network decodes the encoding into an output sequence\n",
    "\n",
    "The encoder-decoder model is a way of organizing recurrent neural networks for sequence-to-sequence prediction problems.\n",
    "\n",
    "# Architecture of Encoder decoder LSTM\n",
    "\n",
    "Input--> Encoder Model-->Decoder Model-->Dense-->output\n",
    "\n",
    "# Application\n",
    "    - Machine Translation\n",
    "    - Learning To Execute\n",
    "    - Image Caption\n",
    "    - Conversational Modeling\n",
    "    - Movement Classification\n",
    "    \n",
    "The approach involves two recurrent neural networks, \n",
    "    1. one to encode the source sequence, called the encoder, \n",
    "    2. second to decode the encoded source sequence into the target sequence, called the decoder.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Life Cycle of LSTM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different Types of LSTM Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked LSTMs : LSTM layers are stacked one on top of another into deep networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional LSTMs : Input sequences are presented and learned both forward and backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative LSTM : LSTMs learn the structure relationship in input sequences so well that they \n",
    "#                   can generate new plausible sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnose and Tune LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update LSTM Models for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
